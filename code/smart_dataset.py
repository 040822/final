import json
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from torch.nn.utils.rnn import pad_sequence
import pickle
import os
import hashlib
import time
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
import numpy as np
from sklearn.model_selection import train_test_split

# å°è¯•å¯¼å…¥ç›¸å…³åº“
try:
    from transformers import AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
    print("âœ“ Transformersåº“å¯ç”¨")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸  Transformersåº“ä¸å¯ç”¨")

try:
    import spacy
    SPACY_AVAILABLE = True
    print("âœ“ SpaCyåº“å¯ç”¨")
except ImportError:
    SPACY_AVAILABLE = False
    print("âš ï¸  SpaCyåº“ä¸å¯ç”¨")

class CachedTranslationDataset(Dataset):
    """æ”¯æŒç¼“å­˜çš„è‹±è¯‘ä¸­ç¿»è¯‘æ•°æ®é›†"""
    
    def __init__(self, data_path, vocab_en=None, vocab_zh=None, max_len=128, 
                 tokenizer_type='transformers', tokenizer_name='bert-base-multilingual-cased',
                 cache_dir='cache', sample_ratio=1.0, min_freq=2):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        Args:
            data_path: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
            vocab_en: è‹±æ–‡è¯æ±‡è¡¨
            vocab_zh: ä¸­æ–‡è¯æ±‡è¡¨
            max_len: æœ€å¤§åºåˆ—é•¿åº¦
            tokenizer_type: åˆ†è¯å™¨ç±»å‹ ('transformers', 'spacy', 'basic')
            tokenizer_name: é¢„è®­ç»ƒåˆ†è¯å™¨åç§°
            cache_dir: ç¼“å­˜ç›®å½•
            sample_ratio: æ•°æ®é‡‡æ ·æ¯”ä¾‹ (0.0-1.0)
            min_freq: æœ€å°è¯é¢‘é˜ˆå€¼
        """
        self.data_path = data_path
        self.max_len = max_len
        self.tokenizer_type = tokenizer_type
        self.tokenizer_name = tokenizer_name
        self.cache_dir = cache_dir
        self.sample_ratio = sample_ratio
        self.min_freq = min_freq
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
        # ç”Ÿæˆç¼“å­˜æ ‡è¯†
        self.cache_key = self._generate_cache_key()
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        self.tokenizer = self._init_tokenizer()
        
        # æ£€æŸ¥ç¼“å­˜æˆ–åŠ è½½æ•°æ®
        if self._load_from_cache():
            print("âœ“ ä»ç¼“å­˜åŠ è½½æ•°æ®æˆåŠŸ")
        else:
            print("ç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼€å§‹å¤„ç†æ•°æ®...")
            self._process_data()
            self._save_to_cache()
        
        # è®¾ç½®è¯æ±‡è¡¨
        if vocab_en is None or vocab_zh is None:
            if not hasattr(self, 'vocab_en'):
                self.vocab_en, self.vocab_zh = self._build_vocab_if_needed()
        else:
            self.vocab_en = vocab_en
            self.vocab_zh = vocab_zh
    
    def _generate_cache_key(self):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_str = f"{os.path.basename(self.data_path)}_{self.max_len}_{self.tokenizer_type}_{self.tokenizer_name}_{self.sample_ratio}_{self.min_freq}"
        return hashlib.md5(key_str.encode()).hexdigest()[:16]
    
    def _init_tokenizer(self):
        """åˆå§‹åŒ–åˆ†è¯å™¨"""
        if self.tokenizer_type == 'transformers' and TRANSFORMERS_AVAILABLE:
            try:
                print(f"åŠ è½½Transformersåˆ†è¯å™¨: {self.tokenizer_name}")
                return AutoTokenizer.from_pretrained(self.tokenizer_name)
            except Exception as e:
                print(f"Transformersåˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
                return None
        elif self.tokenizer_type == 'spacy' and SPACY_AVAILABLE:
            try:
                print("åŠ è½½SpaCyåˆ†è¯å™¨")
                # å°è¯•åŠ è½½å¤šè¯­è¨€æ¨¡å‹
                try:
                    return spacy.load("xx_core_web_sm")  # å¤šè¯­è¨€å°æ¨¡å‹
                except OSError:
                    return spacy.load("en_core_web_sm")  # è‹±æ–‡æ¨¡å‹
            except Exception as e:
                print(f"SpaCyåˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
                return None
        else:
            print("ä½¿ç”¨åŸºç¡€åˆ†è¯å™¨")
            return None
    
    def _load_from_cache(self):
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        cache_file = os.path.join(self.cache_dir, f"dataset_{self.cache_key}.pkl")
        vocab_file = os.path.join(self.cache_dir, f"vocab_{self.cache_key}.pkl")
        
        if os.path.exists(cache_file) and os.path.exists(vocab_file):
            try:
                print("æ­£åœ¨ä»ç¼“å­˜åŠ è½½æ•°æ®...")
                with open(cache_file, 'rb') as f:
                    self.data = pickle.load(f)
                with open(vocab_file, 'rb') as f:
                    vocab_data = pickle.load(f)
                    self.vocab_en = vocab_data['vocab_en']
                    self.vocab_zh = vocab_data['vocab_zh']
                print(f"ç¼“å­˜åŠ è½½å®Œæˆï¼Œå…± {len(self.data)} ä¸ªæ ·æœ¬")
                return True
            except Exception as e:
                print(f"ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
                return False
        return False
    
    def _save_to_cache(self):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        cache_file = os.path.join(self.cache_dir, f"dataset_{self.cache_key}.pkl")
        vocab_file = os.path.join(self.cache_dir, f"vocab_{self.cache_key}.pkl")
        
        try:
            print("æ­£åœ¨ä¿å­˜æ•°æ®åˆ°ç¼“å­˜...")
            with open(cache_file, 'wb') as f:
                pickle.dump(self.data, f)
            with open(vocab_file, 'wb') as f:
                pickle.dump({
                    'vocab_en': self.vocab_en,
                    'vocab_zh': self.vocab_zh
                }, f)
            print("ç¼“å­˜ä¿å­˜å®Œæˆ")
        except Exception as e:
            print(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def _process_data(self):
        """å¤„ç†åŸå§‹æ•°æ®"""
        print(f"å¼€å§‹å¤„ç†æ•°æ®æ–‡ä»¶: {self.data_path}")
        
        # åˆ†å—è¯»å–å¤§æ–‡ä»¶
        raw_data = []
        chunk_size = 10000
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            chunk = []
            for i, line in enumerate(f):
                try:
                    item = json.loads(line.strip())
                    english = item.get('english', '').strip()
                    chinese = item.get('chinese', '').strip()
                    
                    if len(english) > 0 and len(chinese) > 0:
                        chunk.append({'english': english, 'chinese': chinese})
                    
                    if len(chunk) >= chunk_size:
                        raw_data.extend(chunk)
                        chunk = []
                        if i % 50000 == 0:
                            print(f"å·²å¤„ç† {i} è¡Œæ•°æ®...")
                
                except Exception as e:
                    continue
            
            # å¤„ç†æœ€åä¸€å—
            if chunk:
                raw_data.extend(chunk)
        
        print(f"åŸå§‹æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(raw_data)} ä¸ªæ ·æœ¬")
        
        # æ•°æ®é‡‡æ ·
        if self.sample_ratio < 1.0:
            sample_size = int(len(raw_data) * self.sample_ratio)
            raw_data = np.random.choice(raw_data, sample_size, replace=False).tolist()
            print(f"æ•°æ®é‡‡æ ·å®Œæˆï¼Œä¿ç•™ {len(raw_data)} ä¸ªæ ·æœ¬")
        
        # æ„å»ºè¯æ±‡è¡¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.tokenizer_type in ['basic', 'spacy'] or not self.tokenizer:
            self.vocab_en, self.vocab_zh = self._build_vocab(raw_data)
        else:
            # ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨çš„è¯æ±‡è¡¨
            self.vocab_en = dict(self.tokenizer.vocab) if hasattr(self.tokenizer, 'vocab') else {}
            self.vocab_zh = self.vocab_en  # å¤šè¯­è¨€æ¨¡å‹å…±äº«è¯æ±‡è¡¨
        
        # å¹¶è¡Œé¢„å¤„ç†æ•°æ®
        self._preprocess_parallel(raw_data)
    
    def _build_vocab(self, raw_data):
        """æ„å»ºè¯æ±‡è¡¨"""
        print("æ„å»ºè¯æ±‡è¡¨...")
        from collections import Counter
        
        en_counter = Counter()
        zh_counter = Counter()
        
        for item in raw_data[:10000]:  # åªç”¨å‰10000ä¸ªæ ·æœ¬æ„å»ºè¯æ±‡è¡¨ä»¥åŠ é€Ÿ
            en_tokens = self._tokenize_text(item['english'], 'en')
            zh_tokens = self._tokenize_text(item['chinese'], 'zh')
            
            en_counter.update(en_tokens)
            zh_counter.update(zh_tokens)
        
        # åˆ›å»ºè¯æ±‡è¡¨
        vocab_en = self._create_vocab_dict(en_counter)
        vocab_zh = self._create_vocab_dict(zh_counter)
        
        print(f"è‹±æ–‡è¯æ±‡è¡¨å¤§å°: {len(vocab_en)}")
        print(f"ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {len(vocab_zh)}")
        
        return vocab_en, vocab_zh
    
    def _create_vocab_dict(self, counter):
        """åˆ›å»ºè¯æ±‡è¡¨å­—å…¸"""
        vocab = {
            '<pad>': 0,
            '<unk>': 1,
            '<sos>': 2,
            '<eos>': 3
        }
        
        for word, freq in counter.most_common():
            if freq >= self.min_freq:
                vocab[word] = len(vocab)
        
        return vocab
    
    def _tokenize_text(self, text, lang='en'):
        """åˆ†è¯æ–‡æœ¬"""
        if self.tokenizer_type == 'transformers' and self.tokenizer:
            return self.tokenizer.tokenize(text)
        elif self.tokenizer_type == 'spacy' and self.tokenizer:
            doc = self.tokenizer(text)
            return [token.text for token in doc]
        else:
            # åŸºç¡€åˆ†è¯
            if lang == 'zh':
                try:
                    import jieba
                    return list(jieba.cut(text))
                except ImportError:
                    return list(text)  # å­—ç¬¦çº§åˆ†è¯
            else:
                return text.lower().split()
    
    def _preprocess_parallel(self, raw_data):
        """å¹¶è¡Œé¢„å¤„ç†æ•°æ®"""
        print("å¼€å§‹å¹¶è¡Œé¢„å¤„ç†æ•°æ®...")
        
        # åˆ†å—å¤„ç†
        num_workers = min(multiprocessing.cpu_count(), 8)
        chunk_size = len(raw_data) // num_workers
        chunks = [raw_data[i:i+chunk_size] for i in range(0, len(raw_data), chunk_size)]
        
        # å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(self._process_chunk, chunk) for chunk in chunks]
            
            processed_data = []
            for i, future in enumerate(futures):
                chunk_result = future.result()
                processed_data.extend(chunk_result)
                print(f"å®Œæˆç¬¬ {i+1}/{len(futures)} å—å¤„ç†")
        
        self.data = processed_data
        print(f"é¢„å¤„ç†å®Œæˆï¼Œæœ€ç»ˆæ•°æ®é‡: {len(self.data)}")
    
    def _process_chunk(self, chunk):
        """å¤„ç†æ•°æ®å—"""
        processed_chunk = []
        
        for item in chunk:
            try:
                # åˆ†è¯å’Œç¼–ç 
                if self.tokenizer_type == 'transformers' and self.tokenizer:
                    # ä½¿ç”¨Transformersç¼–ç 
                    en_encoding = self.tokenizer.encode_plus(
                        item['english'],
                        max_length=self.max_len,
                        padding=False,
                        truncation=True,
                        return_tensors=None
                    )
                    zh_encoding = self.tokenizer.encode_plus(
                        item['chinese'],
                        max_length=self.max_len,
                        padding=False,
                        truncation=True,
                        return_tensors=None
                    )
                    en_ids = en_encoding['input_ids']
                    zh_ids = zh_encoding['input_ids']
                else:
                    # ä¼ ç»Ÿæ–¹æ³•
                    en_tokens = self._tokenize_text(item['english'], 'en')
                    zh_tokens = self._tokenize_text(item['chinese'], 'zh')
                    
                    en_ids = [self.vocab_en.get(token, self.vocab_en['<unk>']) for token in en_tokens]
                    zh_ids = [self.vocab_zh.get(token, self.vocab_zh['<unk>']) for token in zh_tokens]
                    
                    # æ·»åŠ ç‰¹æ®Šç¬¦å·
                    zh_ids = [self.vocab_zh['<sos>']] + zh_ids + [self.vocab_zh['<eos>']]
                
                # é•¿åº¦æ£€æŸ¥
                if len(en_ids) <= self.max_len and len(zh_ids) <= self.max_len:
                    processed_chunk.append({
                        'english': en_ids,
                        'chinese': zh_ids
                    })
            
            except Exception as e:
                continue
        
        return processed_chunk
    
    def _build_vocab_if_needed(self):
        """å¦‚æœéœ€è¦åˆ™æ„å»ºè¯æ±‡è¡¨"""
        if hasattr(self, 'vocab_en') and hasattr(self, 'vocab_zh'):
            return self.vocab_en, self.vocab_zh
        else:
            # ä»å·²å¤„ç†çš„æ•°æ®æ„å»ºè¯æ±‡è¡¨ï¼ˆåº”è¯¥åœ¨ç¼“å­˜ä¸­ï¼‰
            return self.vocab_en, self.vocab_zh
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

def smart_collate_fn(batch):
    """æ™ºèƒ½æ‰¹å¤„ç†å‡½æ•°"""
    english_batch = [torch.tensor(item['english'], dtype=torch.long) for item in batch]
    chinese_batch = [torch.tensor(item['chinese'], dtype=torch.long) for item in batch]
    
    # åŠ¨æ€å¡«å……
    english_batch = pad_sequence(english_batch, batch_first=True, padding_value=0)
    chinese_batch = pad_sequence(chinese_batch, batch_first=True, padding_value=0)
    
    return {
        'input_ids': english_batch,
        'labels': chinese_batch,
        'attention_mask': (english_batch != 0).long()
    }

def create_smart_dataloaders(data_path, test_size=0.1, batch_size=32, max_len=128,
                           tokenizer_type='transformers', tokenizer_name='bert-base-multilingual-cased',
                           cache_dir='cache', sample_ratio=1.0, num_workers=4):
    """
    åˆ›å»ºæ™ºèƒ½æ•°æ®åŠ è½½å™¨
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        batch_size: æ‰¹å¤§å°
        max_len: æœ€å¤§åºåˆ—é•¿åº¦
        tokenizer_type: åˆ†è¯å™¨ç±»å‹
        tokenizer_name: åˆ†è¯å™¨åç§°
        cache_dir: ç¼“å­˜ç›®å½•
        sample_ratio: æ•°æ®é‡‡æ ·æ¯”ä¾‹
        num_workers: æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
    """
    print("ğŸš€ åˆ›å»ºæ™ºèƒ½æ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºå®Œæ•´æ•°æ®é›†
    full_dataset = CachedTranslationDataset(
        data_path=data_path,
        max_len=max_len,
        tokenizer_type=tokenizer_type,
        tokenizer_name=tokenizer_name,
        cache_dir=cache_dir,
        sample_ratio=sample_ratio
    )
    
    # åˆ†å‰²æ•°æ®é›†
    dataset_size = len(full_dataset)
    test_size_abs = int(dataset_size * test_size)
    train_size_abs = dataset_size - test_size_abs
    
    train_dataset, test_dataset = random_split(
        full_dataset, 
        [train_size_abs, test_size_abs],
        generator=torch.Generator().manual_seed(42)
    )
    
    print(f"âœ“ æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=smart_collate_fn,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
        persistent_workers=True if num_workers > 0 else False
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=smart_collate_fn,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
        persistent_workers=True if num_workers > 0 else False
    )
    
    return train_loader, test_loader, full_dataset.vocab_en, full_dataset.vocab_zh

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    print("=== æµ‹è¯•æ™ºèƒ½æ•°æ®åŠ è½½å™¨ ===")
    
    train_loader, test_loader, vocab_en, vocab_zh = create_smart_dataloaders(
        data_path="data/translation2019zh_train.json",
        test_size=0.1,
        batch_size=4,
        max_len=64,
        tokenizer_type='transformers',
        tokenizer_name='bert-base-multilingual-cased',
        cache_dir='smart_cache',
        sample_ratio=0.01,  # åªç”¨1%çš„æ•°æ®æµ‹è¯•
        num_workers=2
    )
    
    print(f"âœ“ è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"âœ“ æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
    print(f"âœ“ è‹±æ–‡è¯æ±‡è¡¨å¤§å°: {len(vocab_en)}")
    print(f"âœ“ ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {len(vocab_zh)}")
    
    # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
    for i, batch in enumerate(train_loader):
        print(f"âœ“ æ‰¹æ¬¡ {i+1} å½¢çŠ¶:")
        print(f"  input_ids: {batch['input_ids'].shape}")
        print(f"  labels: {batch['labels'].shape}")
        print(f"  attention_mask: {batch['attention_mask'].shape}")
        if i >= 1:  # åªæµ‹è¯•å‰2ä¸ªæ‰¹æ¬¡
            break
    
    print("âœ“ æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆï¼")
