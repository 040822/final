#!/usr/bin/env python3
"""
ç¿»è¯‘æ¨¡å‹è¯„ä¼°è„šæœ¬
ç”¨äºè¯„ä¼°è®­ç»ƒå¥½çš„T5å’ŒBiLSTMæ¨¡å‹
"""

import os
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from datasets import load_dataset
import evaluate
from tqdm import tqdm
import json
import time
from translation_trainer import BiLSTMTranslator

class TranslationEvaluator:
    """ç¿»è¯‘æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, test_data_path="data/translation2019zh_valid.json"):
        self.test_data_path = test_data_path
        self.test_data = None
        self.bleu_metric = evaluate.load("sacrebleu")
        
    def load_test_data(self, sample_size=100):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {self.test_data_path}")
        
        if os.path.exists(self.test_data_path):
            dataset = load_dataset("json", data_files=self.test_data_path)
            self.test_data = dataset["train"]
        else:
            # å¦‚æœæ²¡æœ‰å•ç‹¬çš„æµ‹è¯•æ–‡ä»¶ï¼Œä»è®­ç»ƒæ•°æ®ä¸­é‡‡æ ·
            dataset = load_dataset("json", data_files="data/translation2019zh_train.json")
            self.test_data = dataset["train"].select(range(-sample_size, 0))  # å–æœ€å100ä¸ªæ ·æœ¬
        
        # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡
        if len(self.test_data) > sample_size:
            self.test_data = self.test_data.select(range(sample_size))
        
        print(f"âœ“ æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼Œæ ·æœ¬æ•°: {len(self.test_data)}")
        return self.test_data
    
    def evaluate_t5_model(self, model_path="t5_translation_model"):
        """è¯„ä¼°T5æ¨¡å‹"""
        print(f"\nğŸ” è¯„ä¼°T5æ¨¡å‹: {model_path}")
        
        if not os.path.exists(model_path):
            print(f"âŒ T5æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return None
        
        try:
            # åŠ è½½æ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
            
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model.to(device)
            model.eval()
            
            predictions = []
            references = []
            translation_times = []
            
            print("æ­£åœ¨ç”Ÿæˆç¿»è¯‘...")
            for example in tqdm(self.test_data):
                english_text = example['english']
                chinese_text = example['chinese']
                
                # å‡†å¤‡è¾“å…¥
                input_text = f"translate English to Chinese: {english_text}"
                inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                # ç”Ÿæˆç¿»è¯‘
                start_time = time.time()
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_length=128,
                        num_beams=4,
                        length_penalty=0.6,
                        early_stopping=True
                    )
                end_time = time.time()
                
                # è§£ç ç»“æœ
                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                predictions.append(prediction)
                references.append([chinese_text])
                translation_times.append(end_time - start_time)
            
            # è®¡ç®—BLEUåˆ†æ•°
            bleu_result = self.bleu_metric.compute(predictions=predictions, references=references)
            
            results = {
                'model_type': 'T5',
                'bleu_score': bleu_result['score'],
                'avg_translation_time': np.mean(translation_times),
                'total_samples': len(predictions),
                'predictions': predictions[:5],  # ä¿å­˜å‰5ä¸ªé¢„æµ‹ç»“æœ
                'references': [ref[0] for ref in references[:5]]
            }
            
            print(f"âœ“ T5æ¨¡å‹è¯„ä¼°å®Œæˆ")
            print(f"BLEUåˆ†æ•°: {results['bleu_score']:.4f}")
            print(f"å¹³å‡ç¿»è¯‘æ—¶é—´: {results['avg_translation_time']:.4f}ç§’")
            
            return results
            
        except Exception as e:
            print(f"âŒ T5æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def evaluate_bilstm_model(self, model_path="bilstm_translation_model.pth"):
        """è¯„ä¼°BiLSTMæ¨¡å‹"""
        print(f"\nğŸ” è¯„ä¼°BiLSTMæ¨¡å‹: {model_path}")
        
        if not os.path.exists(model_path):
            print(f"âŒ BiLSTMæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return None
        
        try:
            # åŠ è½½æ¨¡å‹
            checkpoint = torch.load(model_path, map_location='cpu')
            model_config = checkpoint['model_config']
            src_vocab = checkpoint['src_vocab']
            tgt_vocab = checkpoint['tgt_vocab']
            
            # åˆ›å»ºæ¨¡å‹
            model = BiLSTMTranslator(**model_config)
            model.load_state_dict(checkpoint['model_state_dict'])
            
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model.to(device)
            model.eval()
            
            # åˆ›å»ºåå‘è¯æ±‡è¡¨
            src_idx2word = {idx: word for word, idx in src_vocab.items()}
            tgt_idx2word = {idx: word for word, idx in tgt_vocab.items()}
            
            predictions = []
            references = []
            translation_times = []
            
            print("æ­£åœ¨ç”Ÿæˆç¿»è¯‘...")
            for example in tqdm(self.test_data):
                english_text = example['english']
                chinese_text = example['chinese']
                
                # é¢„å¤„ç†è¾“å…¥
                src_tokens = english_text.lower().split()
                src_ids = [src_vocab.get(token, src_vocab['<unk>']) for token in src_tokens]
                src_tensor = torch.tensor([src_ids], dtype=torch.long).to(device)
                
                # ç”Ÿæˆç¿»è¯‘
                start_time = time.time()
                with torch.no_grad():
                    outputs = model(src_tensor, max_length=50)
                    predicted_ids = outputs.argmax(dim=-1).squeeze().cpu().numpy()
                end_time = time.time()
                
                # è§£ç ç»“æœ
                prediction_tokens = []
                for idx in predicted_ids:
                    if idx == tgt_vocab['<eos>']:
                        break
                    if idx in tgt_idx2word:
                        token = tgt_idx2word[idx]
                        if token not in ['<pad>', '<unk>', '<sos>']:
                            prediction_tokens.append(token)
                
                prediction = ''.join(prediction_tokens)
                
                predictions.append(prediction)
                references.append([chinese_text])
                translation_times.append(end_time - start_time)
            
            # è®¡ç®—BLEUåˆ†æ•°
            bleu_result = self.bleu_metric.compute(predictions=predictions, references=references)
            
            results = {
                'model_type': 'BiLSTM',
                'bleu_score': bleu_result['score'],
                'avg_translation_time': np.mean(translation_times),
                'total_samples': len(predictions),
                'predictions': predictions[:5],
                'references': [ref[0] for ref in references[:5]]
            }
            
            print(f"âœ“ BiLSTMæ¨¡å‹è¯„ä¼°å®Œæˆ")
            print(f"BLEUåˆ†æ•°: {results['bleu_score']:.4f}")
            print(f"å¹³å‡ç¿»è¯‘æ—¶é—´: {results['avg_translation_time']:.4f}ç§’")
            
            return results
            
        except Exception as e:
            print(f"âŒ BiLSTMæ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def compare_models(self, t5_results, bilstm_results):
        """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
        print("\n" + "="*60)
        print("æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
        print("="*60)
        
        if t5_results is None and bilstm_results is None:
            print("âŒ æ²¡æœ‰å¯æ¯”è¾ƒçš„æ¨¡å‹ç»“æœ")
            return
        
        # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
        print(f"{'æŒ‡æ ‡':<20} {'T5æ¨¡å‹':<15} {'BiLSTMæ¨¡å‹':<15}")
        print("-" * 50)
        
        if t5_results and bilstm_results:
            print(f"{'BLEUåˆ†æ•°':<20} {t5_results['bleu_score']:<15.4f} {bilstm_results['bleu_score']:<15.4f}")
            print(f"{'å¹³å‡ç¿»è¯‘æ—¶é—´(ç§’)':<20} {t5_results['avg_translation_time']:<15.4f} {bilstm_results['avg_translation_time']:<15.4f}")
            print(f"{'æµ‹è¯•æ ·æœ¬æ•°':<20} {t5_results['total_samples']:<15} {bilstm_results['total_samples']:<15}")
            
            # ç¡®å®šæœ€ä½³æ¨¡å‹
            if t5_results['bleu_score'] > bilstm_results['bleu_score']:
                print(f"\nğŸ† æœ€ä½³æ¨¡å‹: T5 (BLEU: {t5_results['bleu_score']:.4f})")
            else:
                print(f"\nğŸ† æœ€ä½³æ¨¡å‹: BiLSTM (BLEU: {bilstm_results['bleu_score']:.4f})")
        
        elif t5_results:
            print(f"{'BLEUåˆ†æ•°':<20} {t5_results['bleu_score']:<15.4f} {'N/A':<15}")
            print(f"{'å¹³å‡ç¿»è¯‘æ—¶é—´(ç§’)':<20} {t5_results['avg_translation_time']:<15.4f} {'N/A':<15}")
            print(f"{'æµ‹è¯•æ ·æœ¬æ•°':<20} {t5_results['total_samples']:<15} {'N/A':<15}")
            
        elif bilstm_results:
            print(f"{'BLEUåˆ†æ•°':<20} {'N/A':<15} {bilstm_results['bleu_score']:<15.4f}")
            print(f"{'å¹³å‡ç¿»è¯‘æ—¶é—´(ç§’)':<20} {'N/A':<15} {bilstm_results['avg_translation_time']:<15.4f}")
            print(f"{'æµ‹è¯•æ ·æœ¬æ•°':<20} {'N/A':<15} {bilstm_results['total_samples']:<15}")
        
        # æ˜¾ç¤ºç¤ºä¾‹ç¿»è¯‘
        print("\n" + "="*60)
        print("ç¿»è¯‘ç¤ºä¾‹")
        print("="*60)
        
        if t5_results:
            print("\nğŸ“ T5æ¨¡å‹ç¿»è¯‘ç¤ºä¾‹:")
            for i, (pred, ref) in enumerate(zip(t5_results['predictions'], t5_results['references'])):
                print(f"  {i+1}. é¢„æµ‹: {pred}")
                print(f"     å‚è€ƒ: {ref}")
                print()
        
        if bilstm_results:
            print("\nğŸ“ BiLSTMæ¨¡å‹ç¿»è¯‘ç¤ºä¾‹:")
            for i, (pred, ref) in enumerate(zip(bilstm_results['predictions'], bilstm_results['references'])):
                print(f"  {i+1}. é¢„æµ‹: {pred}")
                print(f"     å‚è€ƒ: {ref}")
                print()
    
    def save_results(self, results, filename="evaluation_results.json"):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"âœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {filename}")

def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    print("=== ç¿»è¯‘æ¨¡å‹è¯„ä¼° ===")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    print("ğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨...")
    evaluator = TranslationEvaluator()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("ğŸ“¥ åŠ è½½æµ‹è¯•æ•°æ®...")
    evaluator.load_test_data(sample_size=50)  # ä½¿ç”¨50ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿè¯„ä¼°
    
    # è¯„ä¼°T5æ¨¡å‹
    print("\nğŸ” å¼€å§‹è¯„ä¼°T5æ¨¡å‹...")
    t5_results = evaluator.evaluate_t5_model("t5_translation_model")
    
    # è¯„ä¼°BiLSTMæ¨¡å‹
    print("\nğŸ” å¼€å§‹è¯„ä¼°BiLSTMæ¨¡å‹...")
    bilstm_results = evaluator.evaluate_bilstm_model("bilstm_translation_model.pth")
    
    # æ¯”è¾ƒæ¨¡å‹
    print("\nğŸ“Š æ¯”è¾ƒæ¨¡å‹æ€§èƒ½...")
    evaluator.compare_models(t5_results, bilstm_results)
    
    # ä¿å­˜ç»“æœ
    all_results = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        't5_results': t5_results,
        'bilstm_results': bilstm_results
    }
    
    evaluator.save_results(all_results)
    
    print("\nâœ“ è¯„ä¼°å®Œæˆï¼")

if __name__ == "__main__":
    main()
