{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5df54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设置成功\n",
      "注意：仅限于学术用途，不承诺稳定性保证\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "!source /etc/network_turbo\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "        \n",
    "import os\n",
    "# We need Hugging Face の 镜像网站！\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b246bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained('google/mt5-small')\n",
    "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-small',use_safetensors=True)\n",
    "\n",
    "# the following 2 hyperparameters are task-specific\n",
    "max_source_length = 128 # 512\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee3e616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens_1: ['▁translate', '▁English', '▁to', '▁Chinese', ':', '▁', 'Welcome', '▁to', '▁Beijing']\n",
      "output_tokens_1: ['▁', '欢迎', '来到', '北京']\n",
      "input_tokens 2: ['▁translate', '▁English', '▁to', '▁Chinese', ':', '▁Hu', 'gging', 'Face', '▁is', '▁', 'a', '▁company']\n",
      "output_tokens_2: ['▁', '拥', '抱', '脸', '是一家', '公司']\n",
      "encoding {'input_ids': tensor([[ 37194,   5413,    288,  17542,    267,    259,  29230,    288,  25463,\n",
      "              1,      0,      0,      0],\n",
      "        [ 37194,   5413,    288,  17542,    267,   4691,  30374, 122628,    339,\n",
      "            259,    262,   5835,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Labels: [[259, 72658, 97836, 8467, 1, 0, 0], [259, 117810, 51092, 48211, 229581, 5705, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Suppose we have the following 2 training examples:\n",
    "input_sequence_1 = \"Welcome to Beijing\"\n",
    "output_sequence_1 = \"欢迎来到北京\"\n",
    "\n",
    "input_sequence_2 = \"HuggingFace is a company\"\n",
    "output_sequence_2 = \"拥抱脸是一家公司\"\n",
    "\n",
    "# encode the inputs\n",
    "task_prefix = \"translate English to Chinese: \"\n",
    "input_sequences = [input_sequence_1, input_sequence_2]\n",
    "\n",
    "input_tokens_1 = tokenizer.tokenize(task_prefix + input_sequence_1)\n",
    "print('input_tokens_1:', input_tokens_1)\n",
    "output_tokens_1 = tokenizer.tokenize(output_sequence_1)\n",
    "print('output_tokens_1:', output_tokens_1)\n",
    "\n",
    "input_tokens_2 = tokenizer.tokenize(task_prefix + input_sequence_2)\n",
    "print('input_tokens 2:', input_tokens_2)\n",
    "output_tokens_2 = tokenizer.tokenize(output_sequence_2)\n",
    "print('output_tokens_2:', output_tokens_2)\n",
    "\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\", # pad to the longest sequence in the batch\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print('encoding', encoding)\n",
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "# encode the targets\n",
    "target_encoding = tokenizer(\n",
    "    [output_sequence_1, output_sequence_2], padding=\"longest\", max_length=max_target_length, truncation=True\n",
    ")\n",
    "labels = target_encoding.input_ids\n",
    "\n",
    "print('Labels:', labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878c3385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['<extra_id_0>', '<extra_id_0>']\n"
     ]
    }
   ],
   "source": [
    "# 推理（Inference）\n",
    "# 使用模型对输入进行翻译预测\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_target_length,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# 解码生成的输出\n",
    "predictions = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26d3092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: torch.Size([2, 7])\n",
      "Labels: tensor([[   259,  72658,  97836,   8467,      1,   -100,   -100],\n",
      "        [   259, 117810,  51092,  48211, 229581,   5705,      1]])\n",
      "Predictions: ['<extra_id_0>.', '<extra_id_0>.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 确保有pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_source_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "# 训练数据\n",
    "input_sequence_1 = \"Welcome to Beijing\"\n",
    "output_sequence_1 = \"欢迎来到北京\"\n",
    "input_sequence_2 = \"HuggingFace is a company\"\n",
    "output_sequence_2 = \"拥抱脸是一家公司\"\n",
    "\n",
    "task_prefix = \"translate English to Chinese: \"\n",
    "input_sequences = [input_sequence_1, input_sequence_2]\n",
    "\n",
    "# 编码输入\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "# 正确编码目标（用于训练）\n",
    "target_encoding = tokenizer(\n",
    "    [output_sequence_1, output_sequence_2], \n",
    "    padding=\"longest\", \n",
    "    max_length=max_target_length, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"  # 添加这个\n",
    ")\n",
    "\n",
    "# 创建标签（将pad_token_id替换为-100）\n",
    "labels = target_encoding.input_ids.clone()\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "print('Labels shape:', labels.shape)\n",
    "print('Labels:', labels)\n",
    "\n",
    "# 改进的推理过程\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_target_length,\n",
    "        min_length=3,              # 添加最小长度\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        do_sample=False,           # 确保确定性输出\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# 解码生成的输出\n",
    "predictions = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "477c0c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始快速微调...\n",
      "Epoch 1/20, Loss: 14.7317\n",
      "Epoch 2/20, Loss: 72.5293\n",
      "Epoch 3/20, Loss: 15.7920\n",
      "Epoch 4/20, Loss: 20.7762\n",
      "Epoch 5/20, Loss: 32.1146\n",
      "Epoch 6/20, Loss: 18.5944\n",
      "Epoch 7/20, Loss: 16.9871\n",
      "Epoch 8/20, Loss: 17.8424\n",
      "Epoch 9/20, Loss: 15.7748\n",
      "Epoch 10/20, Loss: 17.7032\n",
      "Epoch 11/20, Loss: 18.0089\n",
      "Epoch 12/20, Loss: 15.7482\n",
      "Epoch 13/20, Loss: 13.5947\n",
      "Epoch 14/20, Loss: 12.8891\n",
      "Epoch 15/20, Loss: 12.3074\n",
      "Epoch 16/20, Loss: 12.6190\n",
      "Epoch 17/20, Loss: 11.3418\n",
      "Epoch 18/20, Loss: 8.5241\n",
      "Epoch 19/20, Loss: 7.5368\n",
      "Epoch 20/20, Loss: 6.8596\n",
      "微调完成!\n",
      "输入: Welcome to Beijing\n",
      "翻译: 你 你 你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你\n",
      "\n",
      "输入: HuggingFace is a company\n",
      "翻译: 你 你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你\n",
      "\n",
      "输入: Hello world\n",
      "翻译: 你 你 你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你你\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 方案3: 快速微调mT5模型\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 准备更多训练数据\n",
    "training_data = [\n",
    "    (\"Welcome to Beijing\", \"欢迎来到北京\"),\n",
    "    (\"HuggingFace is a company\", \"拥抱脸是一家公司\"), \n",
    "    (\"Hello world\", \"你好世界\"),\n",
    "    (\"How are you\", \"你好吗\"),\n",
    "    (\"Thank you\", \"谢谢\"),\n",
    "    (\"Good morning\", \"早上好\"),\n",
    "    (\"I love you\", \"我爱你\"),\n",
    "    (\"What is your name\", \"你叫什么名字\"),\n",
    "]\n",
    "\n",
    "def prepare_batch(data_pairs):\n",
    "    task_prefix = \"translate English to Chinese: \"\n",
    "    inputs = [task_prefix + pair[0] for pair in data_pairs]\n",
    "    targets = [pair[1] for pair in data_pairs]\n",
    "    \n",
    "    # 编码输入\n",
    "    input_encoding = tokenizer(\n",
    "        inputs,\n",
    "        padding=\"longest\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 编码目标\n",
    "    target_encoding = tokenizer(\n",
    "        targets,\n",
    "        padding=\"longest\", \n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 创建标签\n",
    "    labels = target_encoding.input_ids.clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_encoding.input_ids,\n",
    "        'attention_mask': input_encoding.attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# 快速训练\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-1)\n",
    "\n",
    "print(\"开始快速微调...\")\n",
    "for epoch in range(20):  # 增加epoch数\n",
    "    batch = prepare_batch(training_data)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(\n",
    "        input_ids=batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "        labels=batch['labels']\n",
    "    )\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    print(f\"Epoch {epoch+1}/20, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"微调完成!\")\n",
    "\n",
    "# 测试微调后的模型\n",
    "def test_translation(text):\n",
    "    prompt = f\"translate English to Chinese: {text}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=128,\n",
    "            min_length=3,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "# 测试\n",
    "test_sentences = [\"Welcome to Beijing\", \"HuggingFace is a company\", \"Hello world\"]\n",
    "for sentence in test_sentences:\n",
    "    translation = test_translation(sentence)\n",
    "    print(f\"输入: {sentence}\")\n",
    "    print(f\"翻译: {translation}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
