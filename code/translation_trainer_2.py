#!/usr/bin/env python3
"""
è‹±æ–‡åˆ°ä¸­æ–‡ç¿»è¯‘æ¨¡å‹è®­ç»ƒå’Œè¯„ä»·è„šæœ¬
åŸºäºtest.ipynbæ„å»ºï¼ŒåŒ…å«Qwen2.5å’ŒBiLSTMä¸¤ä¸ªbaselineæ¨¡å‹
"""

import os
import subprocess
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer
)
import evaluate
from tqdm import tqdm
import json
import matplotlib.pyplot as plt
from collections import defaultdict
import os
from datasets import load_from_disk
# è®¾ç½®ç¯å¢ƒå˜é‡
def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡"""
    # è®¾ç½®ç½‘ç»œä»£ç†
    result = subprocess.run('bash -c "source /etc/network_turbo && env | grep proxy"', 
                           shell=True, capture_output=True, text=True)
    output = result.stdout
    for line in output.splitlines():
        if '=' in line:
            var, value = line.split('=', 1)
            os.environ[var] = value
    
    # è®¾ç½®Hugging Faceé•œåƒ
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    os.environ["HF_HOME"] = "/root/.cache/huggingface"
    
    print("âœ“ ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")

# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, train_file, valid_file=None, test_size=0.1):
        self.train_file = train_file
        self.valid_file = valid_file
        self.test_size = test_size
        self.datasets = None
        
    def load_data(self):
        """åŠ è½½æ•°æ®é›†"""
        print("ğŸš€ åŠ è½½æ•°æ®é›†...")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        start_time = time.time()
        train_dataset = load_dataset("json", data_files=self.train_file)
        end_time = time.time()
        print(f"âœ“ è®­ç»ƒé›†åŠ è½½å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        # åŠ è½½éªŒè¯æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        #if self.valid_file and os.path.exists(self.valid_file):
        #    valid_dataset = load_dataset("json", data_files=self.valid_file)
        #    self.datasets = {
        #        'train': train_dataset['train'],
        #        'test': valid_dataset['train']
        #    }
        #    print(f"âœ“ éªŒè¯é›†åŠ è½½å®Œæˆ")
        #else:
        
        # ä»è®­ç»ƒé›†åˆ†å‰²
        split_datasets = train_dataset["train"].train_test_split(
            test_size=self.test_size, seed=42
        )
        self.datasets = split_datasets
        print(f"âœ“ æ•°æ®é›†åˆ†å‰²å®Œæˆ")
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(self.datasets['train'])}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(self.datasets['test'])}")
        
        return self.datasets

# Qwenæ¨¡å‹è®­ç»ƒå™¨
class QwenTrainer:
    """Qwenæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_name="Qwen/Qwen2.5-0.5B", max_length=512):
        self.model_name = model_name
        self.max_length = max_length
        self.tokenizer = None
        self.model = None
        self.metric = None
        
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"ğŸ”§ åˆå§‹åŒ–Qwenæ¨¡å‹: {self.model_name}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.metric = evaluate.load("sacrebleu")
        
        print(f"âœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
        
    def preprocess_function(self, examples):
        """é¢„å¤„ç†å‡½æ•° - ä¸ºå› æœè¯­è¨€æ¨¡å‹æ ¼å¼åŒ–"""
        # æ„å»ºç¿»è¯‘æ ¼å¼çš„è¾“å…¥
        texts = []
        for english, chinese in zip(examples["english"], examples["chinese"]):
            # ä½¿ç”¨ç‰¹æ®Šæ ¼å¼æ¥æ ‡è¯†ç¿»è¯‘ä»»åŠ¡
            text = f"Translate English to Chinese:\nEnglish: {english}\nChinese: {chinese}{self.tokenizer.eos_token}"
            texts.append(text)
        
        # åˆ†è¯ - æ·»åŠ paddingä»¥ç¡®ä¿batchä¸­æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´
        tokenized = self.tokenizer(
            texts,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",  # ä½¿ç”¨max_length paddingç¡®ä¿ä¸€è‡´çš„é•¿åº¦
            return_tensors=None
        )
        
        # ä¸ºå› æœè¯­è¨€æ¨¡å‹è®¾ç½®labelsï¼Œç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        tokenized["labels"] = [ids.copy() for ids in tokenized["input_ids"]]
        
        return tokenized

    def preprocess_data(self, datasets, cache_path="qwen_tokenized_datasets", use_cache=True):
        """é¢„å¤„ç†æ•°æ®å¹¶ç¼“å­˜"""
        cache_train_path = f"{cache_path}.train"
        cache_test_path = f"{cache_path}.test"
        
        if os.path.exists(cache_train_path) and os.path.exists(cache_test_path) and use_cache:
            print("ğŸ”„ åŠ è½½å·²ç¼“å­˜çš„é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets = {
                "train": load_from_disk(cache_train_path),
                "test": load_from_disk(cache_test_path)
            }
        else:
            print("ğŸ”„ é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets = datasets.map(
                self.preprocess_function,
                batched=True,
                remove_columns=datasets["train"].column_names,
                desc="Tokenizing"
            )
            print("ğŸ’¾ ç¼“å­˜é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets["train"].save_to_disk(cache_train_path)
            tokenized_datasets["test"].save_to_disk(cache_test_path)
        print("âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆ")
        return tokenized_datasets
    
    def extract_translation(self, text):
        """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–ç¿»è¯‘ç»“æœ"""
        # æŸ¥æ‰¾"Chinese:"åçš„å†…å®¹
        if "Chinese:" in text:
            translation = text.split("Chinese:")[-1].strip()
            # ç§»é™¤å¯èƒ½çš„eos_token
            translation = translation.replace(self.tokenizer.eos_token, "").strip()
            return translation
        return text.strip()
    
    def compute_metrics(self, eval_preds):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        preds, labels = eval_preds
        if isinstance(preds, tuple):
            preds = preds[0]
        
        # è§£ç é¢„æµ‹ç»“æœ
        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)
        
        # è§£ç æ ‡ç­¾
        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)
        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
        
        # æå–ç¿»è¯‘éƒ¨åˆ†
        extracted_preds = [self.extract_translation(pred) for pred in decoded_preds]
        extracted_labels = [self.extract_translation(label) for label in decoded_labels]
        
        # æ ¼å¼åŒ–ä¸ºBLEUè¯„ä¼°æ‰€éœ€çš„æ ¼å¼
        formatted_labels = [[label] for label in extracted_labels]
        
        try:
            result = self.metric.compute(predictions=extracted_preds, references=formatted_labels)
            result = {"bleu": result["score"]}
        except:
            result = {"bleu": 0.0}
        
        prediction_lens = [len(pred.split()) for pred in extracted_preds]
        result["gen_len"] = np.mean(prediction_lens) if prediction_lens else 0
        result = {k: round(v, 4) for k, v in result.items()}
        return result
    
    def train(self, tokenized_datasets, output_dir="qwen_model", epochs=3, batch_size=16):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒQwenæ¨¡å‹...")
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            eval_strategy="epoch",
            learning_rate=2e-5,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            weight_decay=0.01,
            save_total_limit=3,
            num_train_epochs=epochs,
            fp16=False,  # ç¦ç”¨FP16é¿å…æ¢¯åº¦ç¼©æ”¾é—®é¢˜
            bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,  # ä½¿ç”¨BF16å¦‚æœæ”¯æŒ
            logging_dir=f"{output_dir}/logs",
            logging_steps=500,
            save_steps=1000,
            eval_steps=1000,
            report_to=None,
            dataloader_drop_last=True,
            gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœå†…å­˜
            max_grad_norm=1.0,  # æ·»åŠ æ¢¯åº¦è£å‰ª
        )
        
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # å› æœè¯­è¨€æ¨¡å‹
            pad_to_multiple_of=None,  # ç¦ç”¨pad_to_multiple_ofé¿å…é•¿åº¦é—®é¢˜
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_datasets["train"],
            eval_dataset=tokenized_datasets["test"],
            processing_class=self.tokenizer,  # ä¿®å¤å¼ƒç”¨è­¦å‘Š
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
        )
        
        # è®­ç»ƒæ¨¡å‹
        trainer.train()
        
        evaluate_result = trainer.evaluate()
        print("Evaluation Results:", evaluate_result)
        
        # æ‰“å°å‰5è¡Œæ¨ç†ç»“æœ
        self.show_inference_examples(tokenized_datasets["test"])
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"âœ“ Qwenæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜åœ¨: {output_dir}")
        return trainer
    
    def show_inference_examples(self, test_dataset, num_examples=5):
        """æ˜¾ç¤ºæ¨ç†ç¤ºä¾‹"""
        print("\n=== æ¨ç†ç¤ºä¾‹ ===")
        test_samples = test_dataset.select(range(min(num_examples, len(test_dataset))))
        
        for i in range(len(test_samples)):
            sample = test_samples[i]
            input_ids = torch.tensor([sample["input_ids"]]).to(self.model.device)
            
            # æ‰¾åˆ°"Chinese:"çš„ä½ç½®ï¼Œåªä½¿ç”¨åˆ°è¯¥ä½ç½®çš„è¾“å…¥
            input_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
            if "Chinese:" in input_text:
                prompt = input_text.split("Chinese:")[0] + "Chinese:"
                prompt_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.model.device)
            else:
                prompt_ids = input_ids
            
            # ç”Ÿæˆç¿»è¯‘
            with torch.no_grad():
                outputs = self.model.generate(
                    prompt_ids,
                    max_new_tokens=128,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            # è§£ç ç»“æœ
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            original_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
            
            # æå–å„éƒ¨åˆ†
            if "English:" in generated_text and "Chinese:" in generated_text:
                parts = generated_text.split("Chinese:")
                english_part = parts[0].replace("Translate English to Chinese:\nEnglish:", "").strip()
                chinese_part = self.extract_translation(generated_text)
            else:
                english_part = "è§£æå¤±è´¥"
                chinese_part = "è§£æå¤±è´¥"
            
            # æå–å‚è€ƒç¿»è¯‘
            ref_chinese = self.extract_translation(original_text)
            
            print(f"[{i+1}] è‹±æ–‡: {english_part}")
            print(f"    é¢„æµ‹: {chinese_part}")
            print(f"    å‚è€ƒ: {ref_chinese}")
            print()


def train_qwen_model(datasets, model_name="Qwen/Qwen2.5-0.5B", size=0.1, use_cache=True):
    """è®­ç»ƒQwenæ¨¡å‹"""
    print("\n" + "="*50)
    print("è®­ç»ƒQwenæ¨¡å‹")
    print("="*50)

    qwen_trainer = QwenTrainer(model_name=model_name, max_length=256)
    qwen_trainer.setup_model()
    tokenized_datasets = qwen_trainer.preprocess_data(datasets, use_cache=use_cache)
    
    # ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†è¿›è¡Œå¿«é€Ÿè®­ç»ƒ
    small_train = tokenized_datasets["train"].select(range(int(size*len(tokenized_datasets["train"]))))
    small_test = tokenized_datasets["test"].select(range(min(500, len(tokenized_datasets["test"]))))
    small_datasets = {"train": small_train, "test": small_test}
    
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å¤¹åç§°
    safe_model_name = model_name.replace("/", "_").replace(".", "_")
    output_dir = f"qwen/{safe_model_name}_{size}"
    qwen_trainer.train(small_datasets, output_dir=output_dir, epochs=1, batch_size=2)

def scale_law_for_qwen_model(datasets):
    """æµ‹è¯•Qwenæ¨¡å‹çš„ç¼©æ”¾å®šå¾‹"""
    train_qwen_model(datasets, model_name="Qwen/Qwen2.5-0.5B", size=0.001, use_cache=True)
    # train_qwen_model(datasets, model_name="Qwen/Qwen2.5-0.5B", size=0.01, use_cache=True)
    # train_qwen_model(datasets, model_name="Qwen/Qwen2.5-0.5B", size=0.1, use_cache=True)

# ä¸»è®­ç»ƒå‡½æ•°
def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=== è‹±ä¸­ç¿»è¯‘æ¨¡å‹è®­ç»ƒ ===")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # æ•°æ®å¤„ç†
    data_processor = DataProcessor(
        train_file="data/translation2019zh_train1.json",
        valid_file="data/translation2019zh_valid.json",
        test_size=0.01
    )
    
    datasets = data_processor.load_data()
    # train_qwen_model(datasets, model_name="Qwen/Qwen2.5-0.5B", size=0.1)
    scale_law_for_qwen_model(datasets)

    print("\n" + "="*50)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*50)
    print("æ¨¡å‹ä¿å­˜ä½ç½®:")
    print("- Qwenæ¨¡å‹: qwen/")
    print("- BiLSTMæ¨¡å‹: bilstm_translation_model.pth")
    print("- è®­ç»ƒæ›²çº¿: bilstm_training_loss.png")

if __name__ == "__main__":
    main()
