"""
ç³»ç»Ÿæµ‹è¯•è„šæœ¬
éªŒè¯å„ä¸ªæ¨¡å—çš„åŸºæœ¬åŠŸèƒ½
"""
import os
import sys
import json
import torch
import numpy as np
from pathlib import Path
import traceback

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("ğŸ” æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    try:
        # æµ‹è¯•æ ¸å¿ƒæ¨¡å—
        from smart_dataset import CachedTranslationDataset, create_smart_dataloaders
        from models import BiLSTMTranslator, TransformerTranslator, LightweightTranslator
        from utils import setup_logging, calculate_bleu_score, plot_training_curves
        print("âœ“ æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•å¤–éƒ¨ä¾èµ–
        import torch
        import numpy as np
        print(f"âœ“ PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        try:
            from transformers import AutoTokenizer
            print("âœ“ Transformersåº“å¯ç”¨")
        except ImportError:
            print("âš ï¸  Transformersåº“ä¸å¯ç”¨")
        
        try:
            from accelerate import Accelerator
            print("âœ“ Accelerateåº“å¯ç”¨")
        except ImportError:
            print("âš ï¸  Accelerateåº“ä¸å¯ç”¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_models():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("\nğŸ” æµ‹è¯•æ¨¡å‹åˆ›å»º...")
    
    try:
        # æµ‹è¯•BiLSTM
        print("æµ‹è¯•BiLSTMæ¨¡å‹...")
        bilstm_model = BiLSTMTranslator(
            en_vocab_size=1000,
            zh_vocab_size=1000,
            embedding_dim=128,
            hidden_dim=128,
            num_layers=1,
            dropout=0.1,
            max_len=64
        )
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size = 2
        src_len = 10
        tgt_len = 12
        
        src = torch.randint(1, 1000, (batch_size, src_len))
        tgt = torch.randint(1, 1000, (batch_size, tgt_len))
        
        output = bilstm_model(src, tgt)
        print(f"âœ“ BiLSTMè¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æµ‹è¯•Transformer
        print("æµ‹è¯•Transformeræ¨¡å‹...")
        transformer_model = TransformerTranslator(
            en_vocab_size=1000,
            zh_vocab_size=1000,
            d_model=128,
            nhead=4,
            num_layers=2,
            dropout=0.1,
            max_len=64
        )
        
        output = transformer_model(src, tgt)
        print(f"âœ“ Transformerè¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_dataset():
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    print("\nğŸ” æµ‹è¯•æ•°æ®é›†åŠ è½½...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = [
            {"english": "Hello world", "chinese": "ä½ å¥½ä¸–ç•Œ"},
            {"english": "How are you", "chinese": "ä½ å¥½å—"},
            {"english": "Good morning", "chinese": "æ—©ä¸Šå¥½"},
            {"english": "Thank you", "chinese": "è°¢è°¢"},
            {"english": "Good bye", "chinese": "å†è§"}
        ]
        
        test_data_path = "test_data.json"
        with open(test_data_path, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ æµ‹è¯•æ•°æ®å·²åˆ›å»º: {test_data_path}")
        
        # æµ‹è¯•æ•°æ®é›†
        from smart_dataset import CachedTranslationDataset
        
        dataset = CachedTranslationDataset(
            data_path=test_data_path,
            max_len=32,
            tokenizer_type='basic',  # ä½¿ç”¨åŸºç¡€åˆ†è¯å™¨é¿å…ä¾èµ–é—®é¢˜
            cache_dir='test_cache',
            sample_ratio=1.0
        )
        
        print(f"âœ“ æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        sample = dataset[0]
        print(f"âœ“ æ ·æœ¬å½¢çŠ¶: {[x.shape if hasattr(x, 'shape') else len(x) for x in sample]}")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        os.remove(test_data_path)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_utilities():
    """æµ‹è¯•å·¥å…·å‡½æ•°"""
    print("\nğŸ” æµ‹è¯•å·¥å…·å‡½æ•°...")
    
    try:
        from utils import calculate_bleu_score, count_parameters
        
        # æµ‹è¯•BLEUè®¡ç®—
        predictions = [["hello", "world"], ["good", "morning"]]
        references = [[["hello", "world"]], [["good", "morning"]]]
        
        bleu = calculate_bleu_score(predictions, references)
        print(f"âœ“ BLEUåˆ†æ•°è®¡ç®—: {bleu:.4f}")
        
        # æµ‹è¯•å‚æ•°ç»Ÿè®¡
        model = torch.nn.Linear(10, 5)
        params = count_parameters(model)
        print(f"âœ“ å‚æ•°ç»Ÿè®¡: {params}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å·¥å…·å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_training_pipeline():
    """æµ‹è¯•è®­ç»ƒæµç¨‹"""
    print("\nğŸ” æµ‹è¯•è®­ç»ƒæµç¨‹...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
        class MockConfig:
            def __init__(self):
                self.model_type = 'bilstm'
                self.tokenizer_type = 'basic'
                self.batch_size = 2
                self.learning_rate = 0.001
                self.epochs = 1
                self.embedding_dim = 64
                self.hidden_dim = 64
                self.num_layers = 1
                self.dropout = 0.1
                self.max_len = 32
                self.use_accelerate = False
                self.log_dir = 'test_logs'
                self.model_dir = 'test_models'
                self.cache_dir = 'test_cache'
                self.sample_ratio = 1.0
                self.num_workers = 0
                self.gradient_accumulation_steps = 1
                self.max_grad_norm = 1.0
                self.log_interval = 10
                self.early_stopping = 0
        
        config = MockConfig()
        
        # åˆ›å»ºç›®å½•
        Path(config.log_dir).mkdir(exist_ok=True)
        Path(config.model_dir).mkdir(exist_ok=True)
        Path(config.cache_dir).mkdir(exist_ok=True)
        
        print("âœ“ è®­ç»ƒæµç¨‹é…ç½®åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹åˆ›å»º
        from models import BiLSTMTranslator
        
        model = BiLSTMTranslator(
            en_vocab_size=1000,
            zh_vocab_size=1000,
            embedding_dim=config.embedding_dim,
            hidden_dim=config.hidden_dim,
            num_layers=config.num_layers,
            dropout=config.dropout,
            max_len=config.max_len
        )
        
        print("âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
        criterion = torch.nn.CrossEntropyLoss()
        
        print("âœ“ ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°åˆ›å»ºæˆåŠŸ")
        
        # æ¸…ç†æµ‹è¯•ç›®å½•
        import shutil
        if os.path.exists(config.log_dir):
            shutil.rmtree(config.log_dir)
        if os.path.exists(config.model_dir):
            shutil.rmtree(config.model_dir)
        if os.path.exists(config.cache_dir):
            shutil.rmtree(config.cache_dir)
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def check_system_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    print("\nğŸ” æ£€æŸ¥ç³»ç»Ÿè¦æ±‚...")
    
    try:
        # æ£€æŸ¥Pythonç‰ˆæœ¬
        python_version = sys.version_info
        print(f"Pythonç‰ˆæœ¬: {python_version.major}.{python_version.minor}.{python_version.micro}")
        
        if python_version.major < 3 or (python_version.major == 3 and python_version.minor < 8):
            print("âš ï¸  å»ºè®®ä½¿ç”¨Python 3.8æˆ–æ›´é«˜ç‰ˆæœ¬")
        else:
            print("âœ“ Pythonç‰ˆæœ¬æ»¡è¶³è¦æ±‚")
        
        # æ£€æŸ¥PyTorch
        import torch
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥CUDA
        if torch.cuda.is_available():
            print(f"âœ“ CUDAå¯ç”¨ï¼Œç‰ˆæœ¬: {torch.version.cuda}")
            print(f"âœ“ GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        
        # æ£€æŸ¥å†…å­˜
        try:
            import psutil
            memory = psutil.virtual_memory()
            print(f"ç³»ç»Ÿå†…å­˜: {memory.total / 1024**3:.1f}GB (å¯ç”¨: {memory.available / 1024**3:.1f}GB)")
        except ImportError:
            print("âš ï¸  æ— æ³•æ£€æŸ¥ç³»ç»Ÿå†…å­˜ (éœ€è¦å®‰è£…psutil)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿæ£€æŸ¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª è‹±è¯‘ä¸­ç¿»è¯‘ç³»ç»Ÿæµ‹è¯•")
    print("=" * 50)
    
    tests = [
        ("ç³»ç»Ÿè¦æ±‚æ£€æŸ¥", check_system_requirements),
        ("æ¨¡å—å¯¼å…¥æµ‹è¯•", test_imports),
        ("æ¨¡å‹åˆ›å»ºæµ‹è¯•", test_models),
        ("æ•°æ®é›†æµ‹è¯•", test_dataset),
        ("å·¥å…·å‡½æ•°æµ‹è¯•", test_utilities),
        ("è®­ç»ƒæµç¨‹æµ‹è¯•", test_training_pipeline),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ”„ {test_name}...")
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} é€šè¿‡")
            else:
                print(f"âŒ {test_name} å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} å¼‚å¸¸: {e}")
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªã€‚")
        print("\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("1. å‡†å¤‡è®­ç»ƒæ•°æ® (data/translation2019zh_train.json)")
        print("2. è¿è¡Œå¿«é€Ÿè®­ç»ƒ: python launcher.py train bilstm")
        print("3. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: tensorboard --logdir logs")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¾èµ–å’Œé…ç½®ã€‚")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
