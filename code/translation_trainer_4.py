#!/usr/bin/env python3
"""
è‹±æ–‡åˆ°ä¸­æ–‡ç¿»è¯‘æ¨¡å‹è®­ç»ƒå’Œè¯„ä»·è„šæœ¬
åŸºäºtest.ipynbæ„å»ºï¼ŒåŒ…å«T5å’ŒBiLSTMä¸¤ä¸ªbaselineæ¨¡å‹
"""

import os
import subprocess
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM, 
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    T5EncoderModel
)
import evaluate
from tqdm import tqdm
import json
import matplotlib.pyplot as plt
from collections import defaultdict
import os
from datasets import load_from_disk

# è®¾ç½®ç¯å¢ƒå˜é‡
def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡"""
    # è®¾ç½®ç½‘ç»œä»£ç†
    result = subprocess.run('bash -c "source /etc/network_turbo && env | grep proxy"', 
                           shell=True, capture_output=True, text=True)
    output = result.stdout
    for line in output.splitlines():
        if '=' in line:
            var, value = line.split('=', 1)
            os.environ[var] = value
    
    # è®¾ç½®Hugging Faceé•œåƒ
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    os.environ["HF_HOME"] = "/root/.cache/huggingface"
    
    print("âœ“ ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")

# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, train_file, valid_file=None, test_size=0.1):
        self.train_file = train_file
        self.valid_file = valid_file
        self.test_size = test_size
        self.datasets = None
        
    def load_data(self):
        """åŠ è½½æ•°æ®é›†"""
        print("ğŸš€ åŠ è½½æ•°æ®é›†...")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        start_time = time.time()
        train_dataset = load_dataset("json", data_files=self.train_file)
        end_time = time.time()
        print(f"âœ“ è®­ç»ƒé›†åŠ è½½å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        # åŠ è½½éªŒè¯æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        #if self.valid_file and os.path.exists(self.valid_file):
        #    valid_dataset = load_dataset("json", data_files=self.valid_file)
        #    self.datasets = {
        #        'train': train_dataset['train'],
        #        'test': valid_dataset['train']
        #    }
        #    print(f"âœ“ éªŒè¯é›†åŠ è½½å®Œæˆ")
        #else:
        
        # ä»è®­ç»ƒé›†åˆ†å‰²
        split_datasets = train_dataset["train"].train_test_split(
            test_size=self.test_size, seed=42
        )
        self.datasets = split_datasets
        print(f"âœ“ æ•°æ®é›†åˆ†å‰²å®Œæˆ")
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(self.datasets['train'])}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(self.datasets['test'])}")
        
        return self.datasets

# T5æ¨¡å‹è®­ç»ƒå™¨
class T5Trainer:
    """T5æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_name="t5-small", max_length=512):
        self.model_name = model_name
        self.max_length = max_length
        self.tokenizer = None
        self.model = None
        self.metric = None
        
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"ğŸ”§ åˆå§‹åŒ–T5æ¨¡å‹: {self.model_name}")
        if self.model_name=="t5-small":
            tokenizer_name = "google/m"+self.model_name
        else:
            tokenizer_name = self.model_name
        print(f"ä½¿ç”¨åˆ†è¯å™¨: {tokenizer_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=False)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name,use_safetensors=True)
        self.metric = evaluate.load("sacrebleu")
        
        print(f"âœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
        
    def preprocess_function(self, examples):
        """é¢„å¤„ç†å‡½æ•°"""
        inputs = [f"translate English to Chinese: {ex}" for ex in examples["english"]]
        targets = [ex for ex in examples["chinese"]]
        
        model_inputs = self.tokenizer(
            inputs, 
            text_target=targets, 
            max_length=self.max_length, 
            truncation=True,
            padding=False
        )
        return model_inputs

    def preprocess_data(self, datasets, cache_path="t5_tokenized_datasets", use_cache=True):
        """é¢„å¤„ç†æ•°æ®å¹¶ç¼“å­˜"""

        if os.path.exists(f"{cache_path}.train") and os.path.exists(f"{cache_path}.test") and use_cache:
            print("ğŸ”„ åŠ è½½å·²ç¼“å­˜çš„é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets = {
                "train": load_from_disk(f"{cache_path}.train"),
                "test": load_from_disk(f"{cache_path}.test")
            }
        else:
            print("ğŸ”„ é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets = datasets.map(
                self.preprocess_function,
                batched=True,
                remove_columns=datasets["train"].column_names,
                desc="Tokenizing"
            )
            print("ğŸ’¾ ç¼“å­˜é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets["train"].save_to_disk(f"{cache_path}.train")
            tokenized_datasets["test"].save_to_disk(f"{cache_path}.test")
        print("âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆ")
        return tokenized_datasets
    
    def postprocess_text(self, preds, labels):
        """åå¤„ç†æ–‡æœ¬"""
        preds = [pred.strip() for pred in preds]
        labels = [[label.strip()] for label in labels]
        return preds, labels
    
    def compute_metrics(self, eval_preds):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        preds, labels = eval_preds
        if isinstance(preds, tuple):
            preds = preds[0]
        
        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)
        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)
        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
        
        decoded_preds, decoded_labels = self.postprocess_text(decoded_preds, decoded_labels)
        
        result = self.metric.compute(predictions=decoded_preds, references=decoded_labels)
        result = {"bleu": result["score"]}
        
        prediction_lens = [np.count_nonzero(pred != self.tokenizer.pad_token_id) for pred in preds]
        result["gen_len"] = np.mean(prediction_lens)
        result = {k: round(v, 4) for k, v in result.items()}
        return result
    
    def train(self, tokenized_datasets, output_dir="t5_model", epochs=3, batch_size=16):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒT5æ¨¡å‹...")
        
        training_args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            eval_strategy="epoch",
            learning_rate=1e-6,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            weight_decay=0.01,
            save_total_limit=3,
            num_train_epochs=epochs,
            predict_with_generate=True,
            fp16=torch.cuda.is_available(),
            logging_dir=f"{output_dir}/logs",
            logging_steps=10,
            save_steps=1000,
            eval_steps=1000,
            report_to=None,  # ç¦ç”¨wandbç­‰
            max_grad_norm=1.0,
        )
        
        data_collator = DataCollatorForSeq2Seq(self.tokenizer, model=self.model)
        
        trainer = Seq2SeqTrainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_datasets["train"],
            eval_dataset=tokenized_datasets["test"],
            processing_class=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
        )
        
        # è®­ç»ƒæ¨¡å‹
        trainer.train()
        
        #evaluate_result = trainer.evaluate()
        #print("Evaluation Results:", evaluate_result)
        # æ‰“å°å‰5è¡Œæ¨ç†ç»“æœ
        test_dataset = tokenized_datasets["test"]
        test_samples = test_dataset.select(range(5))
            # ä½¿ç”¨data_collatorè¿›è¡Œæ­£ç¡®çš„æ‰¹å¤„ç†
        batch = data_collator([test_samples[i] for i in range(len(test_samples))])

        # è·å–è¾“å…¥å’Œæ ‡ç­¾
        input_ids = batch["input_ids"].to(self.model.device)
        labels = batch["labels"]

        # è§£ç è¾“å…¥
        inputs = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)

        # è§£ç æ ‡ç­¾ï¼ˆå¤„ç†-100ï¼‰
        labels_decoded = []
        for label_seq in labels:
            # å°†-100æ›¿æ¢ä¸ºpad_token_id
            label_seq = torch.where(label_seq != -100, label_seq, self.tokenizer.pad_token_id)
            decoded_label = self.tokenizer.decode(label_seq, skip_special_tokens=True)
            labels_decoded.append(decoded_label)
                # ç”Ÿæˆé¢„æµ‹
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                max_length=self.max_length,
                num_return_sequences=1,
                do_sample=False,
                early_stopping=True
            )

        # è§£ç é¢„æµ‹ç»“æœ
        preds = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
    
        
        # æ‰“å°ç»“æœ
        for i, (inp, pred, label) in enumerate(zip(inputs, preds, labels_decoded)):
            print(f"[{i+1}] è¾“å…¥: {inp}")
            print(f"    é¢„æµ‹: {pred}")
            print(f"    å‚è€ƒ: {label}")
            print()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"âœ“ T5æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜åœ¨: {output_dir}")
        return trainer

def train_t5_model(datasets,model_name="t5-small",size=0.1,use_cache=True):
     # è®­ç»ƒT5æ¨¡å‹
    print("\n" + "="*50)
    print("è®­ç»ƒT5æ¨¡å‹")
    print("="*50)

    t5_trainer = T5Trainer(model_name=model_name, max_length=512)
    t5_trainer.setup_model()
    tokenized_datasets = t5_trainer.preprocess_data(datasets,use_cache=use_cache)
    
    # ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†è¿›è¡Œå¿«é€Ÿè®­ç»ƒ
    small_train = tokenized_datasets["train"].select(range(int(size*len(tokenized_datasets["train"]))))
    small_test = tokenized_datasets["test"].select(range(min(10000, len(tokenized_datasets["test"]))))
    small_datasets = {"train": small_train, "test": small_test}
    output_dir = f"t5/{model_name}_{size}"
    t5_trainer.train(small_datasets, output_dir=output_dir, epochs=1, batch_size=64)
    
def scale_law_for_t5_model(datasets):
    train_t5_model(datasets, model_name="google/mt5-small", size=0.1,use_cache=False)
    #train_t5_model(datasets, model_name="t5-small", size=0.01)
    #train_t5_model(datasets, model_name="t5-small", size=0.1)
    #train_t5_model(datasets, model_name="t5-small", size=1)
    #train_t5_model(datasets, model_name="t5-base", size=0.1)
    #train_t5_model(datasets, model_name="t5-large", size=0.1)
    

# T5æ··åˆæ¨¡å‹è®­ç»ƒå™¨
class T5HybridModel(nn.Module):
    """T5ç¼–ç å™¨ + çº¿æ€§å±‚ + ä¸­æ–‡tokenizerçš„æ··åˆæ¨¡å‹"""
    
    def __init__(self, t5_model_name="t5-small", hidden_dim=512, max_length=128):
        super().__init__()
        self.max_length = max_length
        
        # T5ç¼–ç å™¨
        self.t5_encoder = T5EncoderModel.from_pretrained(t5_model_name)
        self.t5_hidden_size = self.t5_encoder.config.d_model
        
        # ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨T5çš„tokenizerè¿›è¡Œä¸­æ–‡å¤„ç†
        print("ğŸ”§ ä½¿ç”¨T5 tokenizerå¤„ç†ä¸­æ–‡...")
        self.zh_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)
        
        print(f"âœ“ Tokenizerè®¾ç½®å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(self.zh_tokenizer)}")
        
        # è®¾ç½®ç‰¹æ®Štoken ID
        self.pad_token_id = self.zh_tokenizer.pad_token_id
        self.eos_token_id = self.zh_tokenizer.eos_token_id
        
        # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å¼€å§‹token
        # T5ä½¿ç”¨decoder_start_token_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨pad_token_id
        if hasattr(self.zh_tokenizer, 'decoder_start_token_id') and self.zh_tokenizer.decoder_start_token_id is not None:
            self.start_token_id = self.zh_tokenizer.decoder_start_token_id
        else:
            self.start_token_id = self.pad_token_id
        
        vocab_size = len(self.zh_tokenizer)
        
        # ç®€åŒ–çš„è§£ç å™¨æ¶æ„
        self.decoder_embedding = nn.Embedding(vocab_size, hidden_dim)
        self.projection = nn.Linear(self.t5_hidden_size, hidden_dim)
        self.output_projection = nn.Linear(hidden_dim, vocab_size)
        self.dropout = nn.Dropout(0.1)
        
        # ç®€åŒ–çš„è§£ç å™¨
        self.decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(
                d_model=hidden_dim,
                nhead=8,
                dim_feedforward=hidden_dim * 2,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=2  # å‡å°‘å±‚æ•°
        )
        
        # ä½ç½®ç¼–ç 
        self.pos_embedding = nn.Embedding(max_length, hidden_dim)
        
        # ä¿®å¤ï¼šæ­£ç¡®çš„æƒé‡åˆå§‹åŒ–
        self._init_weights()
        
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        # ä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–å€¼
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.padding_idx is not None:
                    nn.init.zeros_(module.weight[module.padding_idx])

    def tokenize_chinese(self, texts):
        """ä¸­æ–‡æ–‡æœ¬tokenization - ä½¿ç”¨T5 tokenizer"""
        if isinstance(texts, str):
            texts = [texts]
        
        tokenized = self.zh_tokenizer(
            texts,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        return tokenized["input_ids"]
    
    def decode_chinese(self, token_ids):
        """è§£ç ä¸­æ–‡token idsä¸ºæ–‡æœ¬"""
        if token_ids.dim() > 1:
            return self.zh_tokenizer.batch_decode(token_ids, skip_special_tokens=True)
        else:
            return self.zh_tokenizer.decode(token_ids, skip_special_tokens=True)
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        """å‰å‘ä¼ æ’­"""
        batch_size = input_ids.size(0)
        device = input_ids.device
        
        # T5ç¼–ç å™¨
        encoder_outputs = self.t5_encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        encoder_hidden = encoder_outputs.last_hidden_state
        
        # æŠ•å½±ç¼–ç å™¨è¾“å‡º
        memory = self.projection(encoder_hidden)
        memory = self.dropout(memory)
        
        if labels is not None:
            # è®­ç»ƒæ¨¡å¼
            # å‡†å¤‡è§£ç å™¨è¾“å…¥ï¼ˆteacher forcingï¼‰
            decoder_input_ids = labels[:, :-1].contiguous()  # å»æ‰æœ€åä¸€ä¸ªtoken
            decoder_targets = labels[:, 1:].contiguous()     # å»æ‰ç¬¬ä¸€ä¸ªtoken
            
            # è¯åµŒå…¥
            tgt_emb = self.decoder_embedding(decoder_input_ids)
            
            # ä½ç½®ç¼–ç 
            seq_len = decoder_input_ids.size(1)
            positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
            pos_emb = self.pos_embedding(positions)
            
            # ç»„åˆåµŒå…¥
            tgt_emb = tgt_emb + pos_emb
            tgt_emb = self.dropout(tgt_emb)
            
            # åˆ›å»ºå› æœæ©ç 
            tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)
            
            # è§£ç å™¨
            memory_key_padding_mask = ~attention_mask.bool() if attention_mask is not None else None
            
            decoder_output = self.decoder(
                tgt=tgt_emb,
                memory=memory,
                tgt_mask=tgt_mask,
                memory_key_padding_mask=memory_key_padding_mask
            )
            
            # è¾“å‡ºæŠ•å½±
            logits = self.output_projection(decoder_output)
            
            # è®¡ç®—æŸå¤±
            loss_fct = nn.CrossEntropyLoss(ignore_index=self.pad_token_id)
            loss = loss_fct(logits.reshape(-1, logits.size(-1)), decoder_targets.reshape(-1))
            
            return {
                'loss': loss,
                'logits': logits
            }
        else:
            # æ¨ç†æ¨¡å¼
            return self.generate(memory, attention_mask)
    
    def generate(self, memory, attention_mask=None, max_length=None):
        """ç”Ÿæˆåºåˆ— - ä¿®å¤ç”Ÿæˆé€»è¾‘"""
        if max_length is None:
            max_length = self.max_length
        
        batch_size = memory.size(0)
        device = memory.device
        
        # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„èµ·å§‹token
        generated = torch.full((batch_size, 1), self.start_token_id, device=device)
        
        memory_key_padding_mask = ~attention_mask.bool() if attention_mask is not None else None
        
        for step in range(max_length - 1):
            current_len = generated.size(1)
            
            # è¯åµŒå…¥
            tgt_emb = self.decoder_embedding(generated)
            
            # ä½ç½®ç¼–ç 
            positions = torch.arange(current_len, device=device).unsqueeze(0).expand(batch_size, -1)
            pos_emb = self.pos_embedding(positions)
            
            # ç»„åˆåµŒå…¥
            tgt_emb = tgt_emb + pos_emb
            
            # å› æœæ©ç 
            tgt_mask = nn.Transformer.generate_square_subsequent_mask(current_len).to(device)
            
            # è§£ç å™¨
            decoder_output = self.decoder(
                tgt=tgt_emb,
                memory=memory,
                tgt_mask=tgt_mask,
                memory_key_padding_mask=memory_key_padding_mask
            )
            
            # è¾“å‡ºæŠ•å½±
            logits = self.output_projection(decoder_output)
            
            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥å¹¶æ·»åŠ æ¸©åº¦æ§åˆ¶
            next_token_logits = logits[:, -1, :] / 0.8  # æ¸©åº¦æ§åˆ¶
            
            # ä¿®å¤ï¼šä½¿ç”¨top-ké‡‡æ ·è€Œä¸æ˜¯è´ªå¿ƒæœç´¢
            top_k = 50
            if top_k > 0:
                # ä¿ç•™top-kä¸ªæœ€å¯èƒ½çš„token
                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)
                # å¯¹å…¶ä»–ä½ç½®è®¾ç½®ä¸ºè´Ÿæ— ç©·
                next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                next_token_logits.scatter_(1, top_k_indices, top_k_logits)
            
            # ä½¿ç”¨softmaxæ¦‚ç‡åˆ†å¸ƒé‡‡æ ·
            probs = F.softmax(next_token_logits, dim=-1)
            next_tokens = torch.multinomial(probs, 1).squeeze(1)
            
            # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
            generated = torch.cat([generated, next_tokens.unsqueeze(1)], dim=1)
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if self.eos_token_id is not None and (next_tokens == self.eos_token_id).all():
                break
            
            # é¿å…æ— é™å¾ªç¯
            if step > 0 and (next_tokens == self.pad_token_id).all():
                break
        
        return generated

class T5HybridTrainer:
    """T5æ··åˆæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, t5_model_name="t5-small", max_length=128):
        self.t5_model_name = t5_model_name
        self.max_length = max_length
        self.en_tokenizer = None
        self.model = None
        self.metric = None
        
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        print(f"ğŸ”§ åˆå§‹åŒ–T5æ··åˆæ¨¡å‹: {self.t5_model_name}")
        
        # è‹±æ–‡tokenizer (T5åŸç”Ÿ)
        self.en_tokenizer = AutoTokenizer.from_pretrained(self.t5_model_name)
        
        # æ··åˆæ¨¡å‹
        self.model = T5HybridModel(
            t5_model_name=self.t5_model_name,
            max_length=self.max_length
        )
        
        # è¯„ä¼°æŒ‡æ ‡
        self.metric = evaluate.load("sacrebleu")
        
        print(f"âœ“ æ··åˆæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def preprocess_function(self, examples):
        """é¢„å¤„ç†å‡½æ•° - ä¿®å¤æ•°æ®å¤„ç†"""
        # è‹±æ–‡è¾“å…¥
        inputs = [f"translate English to Chinese: {ex}" for ex in examples["english"]]
        model_inputs = self.en_tokenizer(
            inputs,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        
        # ä¸­æ–‡æ ‡ç­¾ - ç°åœ¨ä½¿ç”¨ç›¸åŒçš„tokenizer
        targets = examples["chinese"]
        with self.en_tokenizer.as_target_tokenizer():
            labels = self.en_tokenizer(
                targets,
                max_length=self.max_length,
                truncation=True,
                padding="max_length",
                return_tensors="pt"
            )
        
        model_inputs["labels"] = labels["input_ids"]
        
        return model_inputs
    
    def preprocess_data(self, datasets, cache_path="t5_hybrid_tokenized_v2", use_cache=True):
        """é¢„å¤„ç†æ•°æ® - ä¿®å¤æ‰¹å¤„ç†"""
        if os.path.exists(f"{cache_path}.train") and use_cache:
            print("ğŸ”„ åŠ è½½å·²ç¼“å­˜çš„é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets = {
                "train": load_from_disk(f"{cache_path}.train"),
                "test": load_from_disk(f"{cache_path}.test")
            }
        else:
            print("ğŸ”„ é¢„å¤„ç†æ•°æ®...")
            
            def process_batch(examples):
                return self.preprocess_function(examples)
            
            # ä¿®å¤: æ­£ç¡®å¤„ç†datasetså­—å…¸
            tokenized_datasets = {}
            for split in ["train", "test"]:
                if split in datasets:
                    tokenized_datasets[split] = datasets[split].map(
                        process_batch,
                        batched=True,
                        remove_columns=datasets[split].column_names,
                        desc=f"Tokenizing {split}"
                    )
            
            print("ğŸ’¾ ç¼“å­˜é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets["train"].save_to_disk(f"{cache_path}.train")
            tokenized_datasets["test"].save_to_disk(f"{cache_path}.test")
        
        print("âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆ")
        return tokenized_datasets
    
    def train(self, tokenized_datasets, output_dir="t5_hybrid_model_v2", epochs=3, batch_size=8):
        """è®­ç»ƒæ¨¡å‹ - ä¿®å¤è®­ç»ƒå¾ªç¯"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒT5æ··åˆæ¨¡å‹...")
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        self.model.to(device)
        
        # å‡†å¤‡æ•°æ®
        train_dataset = tokenized_datasets["train"]
        test_dataset = tokenized_datasets["test"]
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=self.collate_fn
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=self.collate_fn
        )
        
        # ä¿®å¤ï¼šä½¿ç”¨æ›´åˆé€‚çš„å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨è®¾ç½®
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=epochs)
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float('inf')
        
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            num_batches = 0
            
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
            
            for batch in progress_bar:
                optimizer.zero_grad()
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs["loss"]
                
                # æ£€æŸ¥lossæ˜¯å¦æœ‰æ•ˆ
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"è­¦å‘Š: lossä¸º {loss.item()}, è·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)
                
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    "loss": f"{loss.item():.4f}",
                    "avg_loss": f"{total_loss/num_batches:.4f}"
                })
            
            if num_batches > 0:
                avg_loss = total_loss / num_batches
                print(f"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}")
                
                # å­¦ä¹ ç‡è°ƒåº¦
                scheduler.step()
                
                # ä¿å­˜æœ€å¥½çš„æ¨¡å‹
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    os.makedirs(output_dir, exist_ok=True)
                    torch.save(self.model.state_dict(), f"{output_dir}/best_model.pth")
                
                # æ¯ä¸ªepochåè¯„ä¼°
                if epoch % 1 == 0:
                    self.evaluate(test_loader, device)
            else:
                print(f"Epoch {epoch+1} - æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ‰¹æ¬¡")
        
        print(f"âœ“ T5æ··åˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {output_dir}/best_model.pth")

    def collate_fn(self, batch):
        """æ•°æ®æ•´ç†å‡½æ•°"""
        input_ids = torch.stack([torch.tensor(item["input_ids"]) for item in batch])
        attention_mask = torch.stack([torch.tensor(item["attention_mask"]) for item in batch])
        labels = torch.stack([torch.tensor(item["labels"]) for item in batch])
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    
    def evaluate(self, test_loader, device):
        """è¯„ä¼°æ¨¡å‹ - ä¿®å¤è¯„ä¼°é€»è¾‘"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        sample_predictions = []
        sample_references = []
        
        with torch.no_grad():
            for i, batch in enumerate(tqdm(test_loader, desc="Evaluating")):
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)
                
                # è®¡ç®—æŸå¤±
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                total_loss += outputs["loss"].item()
                num_batches += 1
                
                # æ”¶é›†ä¸€äº›æ ·æœ¬è¿›è¡Œå±•ç¤º
                if i < 3:  # åªå–å‰3ä¸ªbatchçš„æ ·æœ¬
                    # ç”Ÿæˆé¢„æµ‹
                    generated = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    
                    # è§£ç é¢„æµ‹å’Œå‚è€ƒ
                    preds = self.model.decode_chinese(generated[:min(2, generated.size(0))])
                    refs = self.model.decode_chinese(labels[:min(2, labels.size(0))])
                    
                    # è§£ç è¾“å…¥
                    inputs = self.en_tokenizer.batch_decode(input_ids[:min(2, input_ids.size(0))], skip_special_tokens=True)
                    
                    for j, (inp, pred, ref) in enumerate(zip(inputs, preds, refs)):
                        if len(sample_predictions) < 5:  # é™åˆ¶æ ·æœ¬æ•°é‡
                            sample_predictions.append(pred)
                            sample_references.append(ref)
                            print(f"æ ·æœ¬ {len(sample_predictions)}:")
                            print(f"  è¾“å…¥: {inp}")
                            print(f"  é¢„æµ‹: {pred}")
                            print(f"  å‚è€ƒ: {ref}")
                            print()
        
        avg_loss = total_loss / num_batches
        print(f"éªŒè¯æŸå¤±: {avg_loss:.4f}")
        
        # å¦‚æœæœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œè®¡ç®—BLEUåˆ†æ•°
        if len(sample_predictions) > 0:
            try:
                refs_formatted = [[ref] for ref in sample_references]
                result = self.metric.compute(predictions=sample_predictions, references=refs_formatted)
                print(f"BLEU Score (æ ·æœ¬): {result['score']:.4f}")
            except Exception as e:
                print(f"BLEUè®¡ç®—å¤±è´¥: {e}")
        
        self.model.train()

def train_t5_hybrid_model(datasets, size=0.001):  # ä½¿ç”¨æ›´å°çš„æ•°æ®é›†è¿›è¡Œè°ƒè¯•
    """è®­ç»ƒT5æ··åˆæ¨¡å‹"""
    print("\n" + "="*50)
    print("è®­ç»ƒT5æ··åˆæ¨¡å‹ (ä¿®å¤ç‰ˆ v2)")
    print("="*50)
    
    trainer = T5HybridTrainer(t5_model_name="t5-small", max_length=32)  # è¿›ä¸€æ­¥å‡å°‘åºåˆ—é•¿åº¦
    trainer.setup_model()
    
    # ä½¿ç”¨æ›´å°çš„æ•°æ®é›†è¿›è¡Œè°ƒè¯•
    small_train = datasets["train"].select(range(int(size * len(datasets["train"]))))
    small_test = datasets["test"].select(range(min(100, len(datasets["test"]))))
    small_datasets = {"train": small_train, "test": small_test}
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(small_train)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(small_test)}")
    
    tokenized_datasets = trainer.preprocess_data(small_datasets, use_cache=False)
    trainer.train(tokenized_datasets, output_dir="t5_hybrid_model_fixed_v2", epochs=5, batch_size=2)

# ä¸»è®­ç»ƒå‡½æ•°
def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=== è‹±ä¸­ç¿»è¯‘æ¨¡å‹è®­ç»ƒ ===")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # æ•°æ®å¤„ç†
    data_processor = DataProcessor(
        train_file="data/translation2019zh_train1.json",
        valid_file="data/translation2019zh_valid.json",
        test_size=0.01
    )
    
    datasets = data_processor.load_data()
    #train_t5_model(datasets, model_name="t5-small", size=0.1)
    #train_lstm_model(datasets)
    #scale_law_for_t5_model(datasets)
    train_t5_hybrid_model(datasets, size=0.01)

    
    print("\n" + "="*50)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*50)
    print("æ¨¡å‹ä¿å­˜ä½ç½®:")
    print("- T5æ¨¡å‹: t5_translation_model/")
    print("- BiLSTMæ¨¡å‹: bilstm_translation_model.pth")
    print("- è®­ç»ƒæ›²çº¿: bilstm_training_loss.png")

    

# ä¸»è®­ç»ƒå‡½æ•°
def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=== è‹±ä¸­ç¿»è¯‘æ¨¡å‹è®­ç»ƒ ===")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # æ•°æ®å¤„ç†
    data_processor = DataProcessor(
        train_file="data/translation2019zh_train1.json",
        valid_file="data/translation2019zh_valid.json",
        test_size=0.01
    )
    
    datasets = data_processor.load_data()
    #train_t5_model(datasets, model_name="t5-small", size=0.1)
    #train_lstm_model(datasets)
    #scale_law_for_t5_model(datasets)
    train_t5_hybrid_model(datasets, size=0.01)

    
    print("\n" + "="*50)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*50)
    print("æ¨¡å‹ä¿å­˜ä½ç½®:")
    print("- T5æ¨¡å‹: t5_translation_model/")
    print("- BiLSTMæ¨¡å‹: bilstm_translation_model.pth")
    print("- è®­ç»ƒæ›²çº¿: bilstm_training_loss.png")

if __name__ == "__main__":
    main()
