✓ Transformers库可用，将使用预训练分词器
Using device: cuda
Current working directory: /root/final/code
root_dir: /root/final/code
Train path: /root/final/code/data/translation2019zh_train.json
Valid path: /root/final/code/data/translation2019zh_valid.json
Loading data with Transformers tokenizer...
创建数据加载器 - transformers: True
正在加载预训练分词器: bert-base-multilingual-cased
✓ 预训练分词器加载成功
Loading data from /root/final/code/data/translation2019zh_train.json
Loaded 5161434 samples
Building vocabulary...
使用预训练分词器构建词汇表...
Pretrained vocab size: 119547
Preprocessing data...
使用预训练分词器预处理数据...
Preprocessed 5161434 samples
词汇表已保存到: vocab.pkl
正在加载预训练分词器: bert-base-multilingual-cased
✓ 预训练分词器加载成功
Loading data from /root/final/code/data/translation2019zh_valid.json
Loaded 39323 samples
Preprocessing data...
使用预训练分词器预处理数据...
Preprocessed 39323 samples
Creating Transformers-based model...
模型初始化完成:
  - 编码器: bert-base-multilingual-cased
  - 词汇表大小: 119547
  - 编码器维度: 768
  - 解码器维度: 512
Total parameters: 336,770,555
Trainable parameters: 336,770,555
/root/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Starting training with Transformers...

=== Epoch 1/1 ===
Traceback (most recent call last):
  File "/root/final/code/train_with_transformers.py", line 406, in <module>
    main()
  File "/root/final/code/train_with_transformers.py", line 403, in main
    trainer.train(num_epochs=1)
  File "/root/final/code/train_with_transformers.py", line 312, in train
    train_loss = self.train_epoch(epoch)
                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/final/code/train_with_transformers.py", line 257, in train_epoch
    output = self.model(src_input_ids, src_attention_mask, decoder_input)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/final/code/train_with_transformers.py", line 106, in forward
    attended_output, _ = self.attention(
                         ^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1368, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py", line 6097, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py", line 5519, in _in_projection_packed
    kv_proj = linear(k, w_kv, b_kv)
              ^^^^^^^^^^^^^^^^^^^^^
RuntimeError: mat1 and mat2 shapes cannot be multiplied (832x768 and 512x1024)
