#!/usr/bin/env python3
"""
ä¸­æ–‡åˆ°è‹±æ–‡ç¿»è¯‘æ¨¡å‹è®­ç»ƒå’Œè¯„ä»·è„šæœ¬
ä½¿ç”¨mBARTæ¨¡å‹å¹¶ç»˜åˆ¶BLEUå˜åŒ–æ›²çº¿
"""

import os
import subprocess
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM, 
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    MBartForConditionalGeneration,
    MBart50TokenizerFast,
    TrainerCallback
)
import evaluate
from tqdm import tqdm
import json
import matplotlib.pyplot as plt
from collections import defaultdict
import os
from datasets import load_from_disk

# è®¾ç½®ç¯å¢ƒå˜é‡
def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡"""
    # è®¾ç½®ç½‘ç»œä»£ç†
    result = subprocess.run('bash -c "source /etc/network_turbo && env | grep proxy"', 
                           shell=True, capture_output=True, text=True)
    output = result.stdout
    for line in output.splitlines():
        if '=' in line:
            var, value = line.split('=', 1)
            os.environ[var] = value
    
    # è®¾ç½®Hugging Faceé•œåƒ
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    os.environ["HF_HOME"] = "/root/.cache/huggingface"
    
    print("âœ“ ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")

# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, train_file, valid_file=None, test_size=0.1):
        self.train_file = train_file
        self.valid_file = valid_file
        self.test_size = test_size
        self.datasets = None
        
    def load_data(self):
        """åŠ è½½æ•°æ®é›†"""
        print("ğŸš€ åŠ è½½æ•°æ®é›†...")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        start_time = time.time()
        train_dataset = load_dataset("json", data_files=self.train_file)
        end_time = time.time()
        print(f"âœ“ è®­ç»ƒé›†åŠ è½½å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        # åŠ è½½éªŒè¯æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        #if self.valid_file and os.path.exists(self.valid_file):
        #    valid_dataset = load_dataset("json", data_files=self.valid_file)
        #    self.datasets = {
        #        'train': train_dataset['train'],
        #        'test': valid_dataset['train']
        #    }
        #    print(f"âœ“ éªŒè¯é›†åŠ è½½å®Œæˆ")
        #else:
        
        # ä»è®­ç»ƒé›†åˆ†å‰²
        split_datasets = train_dataset["train"].train_test_split(
            test_size=self.test_size, seed=42
        )
        self.datasets = split_datasets
        print(f"âœ“ æ•°æ®é›†åˆ†å‰²å®Œæˆ")
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(self.datasets['train'])}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(self.datasets['test'])}")
        
        return self.datasets

# è‡ªå®šä¹‰å›è°ƒå‡½æ•°æ¥è®°å½•BLEUåˆ†æ•°
class BleuTrackingCallback(TrainerCallback):
    """è¿½è¸ªè®­ç»ƒè¿‡ç¨‹ä¸­BLEUåˆ†æ•°å˜åŒ–çš„å›è°ƒå‡½æ•°"""
    
    def __init__(self):
        self.bleu_scores = []
        self.steps = []
        self.epochs = []
        
    def on_evaluate(self, args, state, control, model=None, eval_dataloader=None, **kwargs):
        """åœ¨æ¯æ¬¡è¯„ä¼°åè®°å½•BLEUåˆ†æ•°"""
        if hasattr(state, 'log_history'):
            for log in state.log_history:
                if 'eval_bleu' in log:
                    self.bleu_scores.append(log['eval_bleu'])
                    self.steps.append(log.get('step', len(self.bleu_scores)))
                    self.epochs.append(log.get('epoch', len(self.bleu_scores)))
                    break
    
    def save_bleu_curve(self, output_dir):
        """ä¿å­˜BLEUåˆ†æ•°å˜åŒ–æ›²çº¿"""
        if not self.bleu_scores:
            print("âš ï¸ æ²¡æœ‰BLEUåˆ†æ•°æ•°æ®å¯ç»˜åˆ¶")
            return
            
        plt.figure(figsize=(12, 8))
        
        # ç»˜åˆ¶BLEUåˆ†æ•°éšè®­ç»ƒæ­¥æ•°çš„å˜åŒ–
        plt.subplot(2, 1, 1)
        plt.plot(self.steps, self.bleu_scores, 'b-o', linewidth=2, markersize=4)
        plt.title('BLEUåˆ†æ•°éšè®­ç»ƒæ­¥æ•°å˜åŒ–', fontsize=14, fontweight='bold')
        plt.xlabel('è®­ç»ƒæ­¥æ•°')
        plt.ylabel('BLEUåˆ†æ•°')
        plt.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶BLEUåˆ†æ•°éšepochçš„å˜åŒ–
        plt.subplot(2, 1, 2)
        plt.plot(self.epochs, self.bleu_scores, 'r-s', linewidth=2, markersize=4)
        plt.title('BLEUåˆ†æ•°éšè®­ç»ƒè½®æ¬¡å˜åŒ–', fontsize=14, fontweight='bold')
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('BLEUåˆ†æ•°')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        os.makedirs(output_dir, exist_ok=True)
        plt.savefig(f"{output_dir}/bleu_curve.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # ä¿å­˜æ•°æ®
        bleu_data = {
            'steps': self.steps,
            'epochs': self.epochs,
            'bleu_scores': self.bleu_scores
        }
        with open(f"{output_dir}/bleu_data.json", 'w', encoding='utf-8') as f:
            json.dump(bleu_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ BLEUå˜åŒ–æ›²çº¿å·²ä¿å­˜åˆ° {output_dir}/bleu_curve.png")
        print(f"âœ“ BLEUæ•°æ®å·²ä¿å­˜åˆ° {output_dir}/bleu_data.json")

# mBARTæ¨¡å‹è®­ç»ƒå™¨
class MBartTrainer:
    """mBARTæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_name="facebook/mbart-large-50", max_length=512):
        self.model_name = model_name
        self.max_length = max_length
        self.tokenizer = None
        self.model = None
        self.metric = None
        self.bleu_callback = None
        
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"ğŸ”§ åˆå§‹åŒ–mBARTæ¨¡å‹: {self.model_name}")
        
        # åŠ è½½mBARTä¸“ç”¨çš„tokenizerå’Œmodel
        self.tokenizer = MBart50TokenizerFast.from_pretrained(self.model_name)
        self.model = MBartForConditionalGeneration.from_pretrained(self.model_name,use_safetensors=True)
        
        # è®¾ç½®æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ (cc25ç‰ˆæœ¬ä½¿ç”¨ä¸åŒçš„è¯­è¨€ä»£ç )
        self.tokenizer.src_lang = "zh_CN" 
        self.tokenizer.tgt_lang = "en_XX"
        
        self.metric = evaluate.load("sacrebleu")
        self.bleu_callback = BleuTrackingCallback()
        
        print(f"âœ“ mBARTæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
        print(f"æºè¯­è¨€: {self.tokenizer.src_lang}")
        print(f"ç›®æ ‡è¯­è¨€: {self.tokenizer.tgt_lang}")
        
    def preprocess_function(self, examples):
        """é¢„å¤„ç†å‡½æ•°"""
        # mBARTä¸éœ€è¦ç‰¹æ®Šçš„promptï¼Œç›´æ¥ä½¿ç”¨åŸæ–‡
        inputs = examples["chinese"]
        targets = examples["english"]
        
        # è®¾ç½®æºè¯­è¨€
        self.tokenizer.src_lang = "zh_CN"
        model_inputs = self.tokenizer(
            inputs, 
            max_length=self.max_length, 
            truncation=True,
            padding=False
        )
        
        # è®¾ç½®ç›®æ ‡è¯­è¨€å¹¶ç¼–ç ç›®æ ‡æ–‡æœ¬
        self.tokenizer.tgt_lang = "en_XX"
        with self.tokenizer.as_target_tokenizer():
            labels = self.tokenizer(
                targets, 
                max_length=self.max_length, 
                truncation=True,
                padding=False
            )
        
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    def preprocess_data(self, datasets, cache_path="mbart_tokenized_datasets", use_cache=True):
        """é¢„å¤„ç†æ•°æ®å¹¶ç¼“å­˜"""
        if os.path.exists(f"{cache_path}.train") and os.path.exists(f"{cache_path}.test") and use_cache:
            print("ğŸ”„ åŠ è½½å·²ç¼“å­˜çš„é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets = {
                "train": load_from_disk(f"{cache_path}.train"),
                "test": load_from_disk(f"{cache_path}.test")
            }
        else:
            print("ğŸ”„ é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets = datasets.map(
                self.preprocess_function,
                batched=True,
                remove_columns=datasets["train"].column_names,
                desc="Tokenizing for mBART"
            )
            print("ğŸ’¾ ç¼“å­˜é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets["train"].save_to_disk(f"{cache_path}.train")
            tokenized_datasets["test"].save_to_disk(f"{cache_path}.test")
        print("âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆ")
        return tokenized_datasets
    
    def postprocess_text(self, preds, labels):
        """åå¤„ç†æ–‡æœ¬"""
        preds = [pred.strip() for pred in preds]
        labels = [[label.strip()] for label in labels]
        return preds, labels
    
    def compute_metrics(self, eval_preds):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        preds, labels = eval_preds
        if isinstance(preds, tuple):
            preds = preds[0]
        
        # è§£ç é¢„æµ‹ç»“æœ
        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)
        
        # å¤„ç†æ ‡ç­¾
        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)
        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
        
        decoded_preds, decoded_labels = self.postprocess_text(decoded_preds, decoded_labels)
        for i in range(5):
            # æ‰“å°å‰5ä¸ªé¢„æµ‹å’Œæ ‡ç­¾ç»“æœ
            print(f"[{i+1}] é¢„æµ‹: {decoded_preds[i]}")
            print(f"    æ ‡ç­¾: {decoded_labels[i]}") 
        
        # è®¡ç®—BLEUåˆ†æ•°
        result = self.metric.compute(predictions=decoded_preds, references=decoded_labels)
        result = {"bleu": result["score"]}
        
        prediction_lens = [np.count_nonzero(pred != self.tokenizer.pad_token_id) for pred in preds]
        result["gen_len"] = np.mean(prediction_lens)
        result = {k: round(v, 4) for k, v in result.items()}
        
        
        return result
    
    def train(self, tokenized_datasets, output_dir="mbart_model", epochs=3, batch_size=64):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒmBARTæ¨¡å‹...")
        
        training_args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            eval_strategy="steps",  # æ”¹ä¸ºæŒ‰æ­¥æ•°è¯„ä¼°
            eval_steps=100,  # æ¯100æ­¥è¯„ä¼°ä¸€æ¬¡
            learning_rate=3e-5,  # mBARTæ¨èçš„å­¦ä¹ ç‡
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            weight_decay=0.01,
            save_total_limit=1,
            num_train_epochs=epochs,
            predict_with_generate=True,
            fp16=torch.cuda.is_available(),
            logging_dir=f"{output_dir}/logs",
            logging_steps=100,
            save_steps=1000,
            report_to=None,
            load_best_model_at_end=True,
            metric_for_best_model="bleu",
            greater_is_better=True,
            warmup_steps=500,  # é¢„çƒ­æ­¥æ•°
        )
        
        data_collator = DataCollatorForSeq2Seq(
            self.tokenizer, 
            model=self.model,
            padding=True
        )
        
        trainer = Seq2SeqTrainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_datasets["train"],
            eval_dataset=tokenized_datasets["test"],
            processing_class=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
            callbacks=[self.bleu_callback],  # æ·»åŠ BLEUè¿½è¸ªå›è°ƒ
        )
        
        
        evaluate_result=trainer.evaluate()  # åˆå§‹è¯„ä¼°
        print("ğŸ”„ åˆå§‹è¯„ä¼°ç»“æœ:")
        print(evaluate_result)
               
        # æ‰“å°å‰5è¡Œæ¨ç†ç»“æœ
        print("\n" + "="*50)
        print("æ¨¡å‹æ¨ç†ç¤ºä¾‹")
        print("="*50)
        
        test_dataset = tokenized_datasets["test"]
        test_samples = test_dataset.select(range(5))
        batch = data_collator([test_samples[i] for i in range(len(test_samples))])

        # è·å–è¾“å…¥å’Œæ ‡ç­¾
        input_ids = batch["input_ids"].to(self.model.device)
        labels = batch["labels"]

        # è§£ç è¾“å…¥
        inputs = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)

        # è§£ç æ ‡ç­¾ï¼ˆå¤„ç†-100ï¼‰
        labels_decoded = []
        for label_seq in labels:
            label_seq = torch.where(label_seq != -100, label_seq, self.tokenizer.pad_token_id)
            decoded_label = self.tokenizer.decode(label_seq, skip_special_tokens=True)
            labels_decoded.append(decoded_label)
            
        # ç”Ÿæˆé¢„æµ‹
        with torch.no_grad():
            # è®¾ç½®ç›®æ ‡è¯­è¨€token
            generated_tokens = self.model.generate(
                input_ids,
                forced_bos_token_id=self.tokenizer.lang_code_to_id["en_XX"],
                max_length=self.max_length,
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=2
            )

        # è§£ç é¢„æµ‹ç»“æœ
        preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        
        # æ‰“å°ç»“æœ
        for i, (inp, pred, label) in enumerate(zip(inputs, preds, labels_decoded)):
            print(f"[{i+1}] è¾“å…¥: {inp}")
            print(f"    é¢„æµ‹: {pred}")
            print(f"    å‚è€ƒ: {label}")
            print()
            
        
        # è®­ç»ƒæ¨¡å‹
        trainer.train()
        
        # ä¿å­˜BLEUå˜åŒ–æ›²çº¿
        self.bleu_callback.save_bleu_curve(output_dir)
        
        # æ‰“å°å‰5è¡Œæ¨ç†ç»“æœ
        print("\n" + "="*50)
        print("æ¨¡å‹æ¨ç†ç¤ºä¾‹")
        print("="*50)
        
        test_dataset = tokenized_datasets["test"]
        test_samples = test_dataset.select(range(5))
        batch = data_collator([test_samples[i] for i in range(len(test_samples))])

        # è·å–è¾“å…¥å’Œæ ‡ç­¾
        input_ids = batch["input_ids"].to(self.model.device)
        labels = batch["labels"]

        # è§£ç è¾“å…¥
        inputs = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)

        # è§£ç æ ‡ç­¾ï¼ˆå¤„ç†-100ï¼‰
        labels_decoded = []
        for label_seq in labels:
            label_seq = torch.where(label_seq != -100, label_seq, self.tokenizer.pad_token_id)
            decoded_label = self.tokenizer.decode(label_seq, skip_special_tokens=True)
            labels_decoded.append(decoded_label)
            
        # ç”Ÿæˆé¢„æµ‹
        with torch.no_grad():
            # cc25ç‰ˆæœ¬ä½¿ç”¨ä¸åŒçš„å¼ºåˆ¶å¼€å§‹token
            generated_tokens = self.model.generate(
                input_ids,
                forced_bos_token_id=self.tokenizer.lang_code_to_id["en_XX"],
                max_length=self.max_length,
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=2
            )

        # è§£ç é¢„æµ‹ç»“æœ
        preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        
        # æ‰“å°ç»“æœ
        for i, (inp, pred, label) in enumerate(zip(inputs, preds, labels_decoded)):
            print(f"[{i+1}] è¾“å…¥: {inp}")
            print(f"    é¢„æµ‹: {pred}")
            print(f"    å‚è€ƒ: {label}")
            print()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"âœ“ mBARTæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜åœ¨: {output_dir}")
        return trainer

def train_mbart_model(datasets, model_name="facebook/mbart-large-50", size=0.1, use_cache=True):
    """è®­ç»ƒmBARTæ¨¡å‹"""
    print("\n" + "="*50)
    print("è®­ç»ƒmBARTæ¨¡å‹")
    print("="*50)

    mbart_trainer = MBartTrainer(model_name=model_name, max_length=512)
    mbart_trainer.setup_model()
    tokenized_datasets = mbart_trainer.preprocess_data(datasets, use_cache=use_cache)
    
    # ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†è¿›è¡Œå¿«é€Ÿè®­ç»ƒ
    small_train = tokenized_datasets["train"].select(range(int(size*len(tokenized_datasets["train"]))))
    small_test = tokenized_datasets["test"].select(range(min(500, len(tokenized_datasets["test"]))))
    small_datasets = {"train": small_train, "test": small_test}
    
    model_name_clean = model_name.replace("/", "_")
    output_dir = f"mbart/{model_name_clean}_{size}"
    
    mbart_trainer.train(small_datasets, output_dir=output_dir, epochs=3, batch_size=64)
    
    return output_dir

def scale_law_for_mbart_model(datasets):
    """mBARTæ¨¡å‹çš„è§„æ¨¡å®šå¾‹å®éªŒ"""
    print("\n" + "="*50)
    print("mBARTæ¨¡å‹è§„æ¨¡å®šå¾‹å®éªŒ")
    print("="*50)
    
    # ä¸åŒæ•°æ®é›†å¤§å°çš„å®éªŒ
    sizes = [0.01, 0.1]
    results = {}
    
    for size in sizes:
        print(f"\nğŸ”„ è®­ç»ƒæ•°æ®é›†å¤§å°: {size*100:.1f}%")
        output_dir = train_mbart_model(
            datasets, 
            model_name="facebook/mbart-large-50", 
            size=size, 
            use_cache=False
        )
        results[f"size_{size}"] = output_dir
    
    # ç»˜åˆ¶è§„æ¨¡å®šå¾‹å›¾è¡¨
    plot_scaling_results(results, sizes)
    
    return results

def plot_scaling_results(results, sizes):
    """ç»˜åˆ¶è§„æ¨¡å®šå¾‹ç»“æœ"""
    print("\nğŸ”„ ç»˜åˆ¶è§„æ¨¡å®šå¾‹ç»“æœ...")
    
    final_bleu_scores = []
    
    for size in sizes:
        result_key = f"size_{size}"
        if result_key in results:
            bleu_file = f"{results[result_key]}/bleu_data.json"
            if os.path.exists(bleu_file):
                with open(bleu_file, 'r', encoding='utf-8') as f:
                    bleu_data = json.load(f)
                    if bleu_data['bleu_scores']:
                        final_bleu_scores.append(max(bleu_data['bleu_scores']))
                    else:
                        final_bleu_scores.append(0)
            else:
                final_bleu_scores.append(0)
        else:
            final_bleu_scores.append(0)
    
    # ç»˜åˆ¶è§„æ¨¡å®šå¾‹å›¾è¡¨
    plt.figure(figsize=(10, 6))
    plt.plot([s*100 for s in sizes], final_bleu_scores, 'bo-', linewidth=2, markersize=8)
    plt.title('mBARTæ¨¡å‹æ€§èƒ½éšæ•°æ®é›†å¤§å°å˜åŒ–', fontsize=14, fontweight='bold')
    plt.xlabel('è®­ç»ƒæ•°æ®ç™¾åˆ†æ¯” (%)')
    plt.ylabel('æœ€ä½³BLEUåˆ†æ•°')
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (size, score) in enumerate(zip(sizes, final_bleu_scores)):
        plt.annotate(f'{score:.2f}', 
                    (size*100, score), 
                    textcoords="offset points", 
                    xytext=(0,10), 
                    ha='center')
    
    plt.tight_layout()
    plt.savefig("mbart/scaling_law_results.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ“ è§„æ¨¡å®šå¾‹å›¾è¡¨å·²ä¿å­˜åˆ° mbart/scaling_law_results.png")

# ä¸»è®­ç»ƒå‡½æ•°
def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=== ä¸­è‹±ç¿»è¯‘æ¨¡å‹è®­ç»ƒ (mBART) ===")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # æ•°æ®å¤„ç†
    data_processor = DataProcessor(
        train_file="data/translation2019zh_train1.json",
        valid_file="data/translation2019zh_valid.json",
        test_size=0.01
    )
    
    datasets = data_processor.load_data()
    
    # è¿è¡ŒmBARTè§„æ¨¡å®šå¾‹å®éªŒ
    scale_law_for_mbart_model(datasets)

    print("\n" + "="*50)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*50)
    print("æ¨¡å‹ä¿å­˜ä½ç½®:")
    print("- mBARTæ¨¡å‹: mbart/")
    print("- BLEUå˜åŒ–æ›²çº¿: mbart/*/bleu_curve.png")
    print("- è§„æ¨¡å®šå¾‹ç»“æœ: mbart/scaling_law_results.png")

if __name__ == "__main__":
    main()
