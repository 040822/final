#!/usr/bin/env python3
"""
è‹±æ–‡åˆ°ä¸­æ–‡ç¿»è¯‘æ¨¡å‹è®­ç»ƒå’Œè¯„ä»·è„šæœ¬
åŸºäºtest.ipynbæ„å»ºï¼ŒåŒ…å«T5å’ŒBiLSTMä¸¤ä¸ªbaselineæ¨¡å‹
"""

import os
import subprocess
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM, 
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)
import evaluate
from tqdm import tqdm
import json
import matplotlib.pyplot as plt
from collections import defaultdict
import os
from datasets import load_from_disk
# è®¾ç½®ç¯å¢ƒå˜é‡
def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡"""
    # è®¾ç½®ç½‘ç»œä»£ç†
    result = subprocess.run('bash -c "source /etc/network_turbo && env | grep proxy"', 
                           shell=True, capture_output=True, text=True)
    output = result.stdout
    for line in output.splitlines():
        if '=' in line:
            var, value = line.split('=', 1)
            os.environ[var] = value
    
    # è®¾ç½®Hugging Faceé•œåƒ
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    os.environ["HF_HOME"] = "/root/.cache/huggingface"
    
    print("âœ“ ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")

# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, train_file, valid_file=None, test_size=0.1):
        self.train_file = train_file
        self.valid_file = valid_file
        self.test_size = test_size
        self.datasets = None
        
    def load_data(self):
        """åŠ è½½æ•°æ®é›†"""
        print("ğŸš€ åŠ è½½æ•°æ®é›†...")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        start_time = time.time()
        train_dataset = load_dataset("json", data_files=self.train_file)
        end_time = time.time()
        print(f"âœ“ è®­ç»ƒé›†åŠ è½½å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        # åŠ è½½éªŒè¯æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        #if self.valid_file and os.path.exists(self.valid_file):
        #    valid_dataset = load_dataset("json", data_files=self.valid_file)
        #    self.datasets = {
        #        'train': train_dataset['train'],
        #        'test': valid_dataset['train']
        #    }
        #    print(f"âœ“ éªŒè¯é›†åŠ è½½å®Œæˆ")
        #else:
        
        # ä»è®­ç»ƒé›†åˆ†å‰²
        split_datasets = train_dataset["train"].train_test_split(
            test_size=self.test_size, seed=42
        )
        self.datasets = split_datasets
        print(f"âœ“ æ•°æ®é›†åˆ†å‰²å®Œæˆ")
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(self.datasets['train'])}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(self.datasets['test'])}")
        
        return self.datasets

# T5æ¨¡å‹è®­ç»ƒå™¨
class T5Trainer:
    """T5æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_name="t5-small", max_length=512):
        self.model_name = model_name
        self.max_length = max_length
        self.tokenizer = None
        self.model = None
        self.metric = None
        
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"ğŸ”§ åˆå§‹åŒ–T5æ¨¡å‹: {self.model_name}")
        tokenizer_name = "google/mt5-base"
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
        self.metric = evaluate.load("sacrebleu")
        
        print(f"âœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
        
    def preprocess_function(self, examples):
        """é¢„å¤„ç†å‡½æ•°"""
        inputs = [f"translate English to Chinese: {ex}" for ex in examples["english"]]
        targets = [ex for ex in examples["chinese"]]
        
        model_inputs = self.tokenizer(
            inputs, 
            text_target=targets, 
            max_length=self.max_length, 
            truncation=True,
            padding=False
        )
        return model_inputs
    
    def preprocess_data(self, datasets, cache_path="t5_tokenized_datasets"):
        """é¢„å¤„ç†æ•°æ®å¹¶ç¼“å­˜"""

        if os.path.exists(f"{cache_path}.train") and os.path.exists(f"{cache_path}.test"):
            print("ğŸ”„ åŠ è½½å·²ç¼“å­˜çš„é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets = {
                "train": load_from_disk(f"{cache_path}.train"),
                "test": load_from_disk(f"{cache_path}.test")
            }
        else:
            print("ğŸ”„ é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets = datasets.map(
                self.preprocess_function,
                batched=True,
                remove_columns=datasets["train"].column_names,
                desc="Tokenizing"
            )
            print("ğŸ’¾ ç¼“å­˜é¢„å¤„ç†æ•°æ®...")
            tokenized_datasets["train"].save_to_disk(f"{cache_path}.train")
            tokenized_datasets["test"].save_to_disk(f"{cache_path}.test")
        print("âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆ")
        return tokenized_datasets
    
    def postprocess_text(self, preds, labels):
        """åå¤„ç†æ–‡æœ¬"""
        preds = [pred.strip() for pred in preds]
        labels = [[label.strip()] for label in labels]
        return preds, labels
    
    def compute_metrics(self, eval_preds):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        preds, labels = eval_preds
        if isinstance(preds, tuple):
            preds = preds[0]
        
        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)
        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)
        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
        
        decoded_preds, decoded_labels = self.postprocess_text(decoded_preds, decoded_labels)
        
        result = self.metric.compute(predictions=decoded_preds, references=decoded_labels)
        result = {"bleu": result["score"]}
        
        prediction_lens = [np.count_nonzero(pred != self.tokenizer.pad_token_id) for pred in preds]
        result["gen_len"] = np.mean(prediction_lens)
        result = {k: round(v, 4) for k, v in result.items()}
        return result
    
    def train(self, tokenized_datasets, output_dir="t5_model", epochs=3, batch_size=16):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒT5æ¨¡å‹...")
        
        training_args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            eval_strategy="epoch",
            learning_rate=2e-5,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            weight_decay=0.01,
            save_total_limit=3,
            num_train_epochs=epochs,
            predict_with_generate=True,
            fp16=torch.cuda.is_available(),
            logging_dir=f"{output_dir}/logs",
            logging_steps=500,
            save_steps=1000,
            eval_steps=1000,
            report_to=None,  # ç¦ç”¨wandbç­‰
        )
        
        data_collator = DataCollatorForSeq2Seq(self.tokenizer, model=self.model)
        
        trainer = Seq2SeqTrainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_datasets["train"],
            eval_dataset=tokenized_datasets["test"],
            processing_class=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
        )
        
        # è®­ç»ƒæ¨¡å‹
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"âœ“ T5æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜åœ¨: {output_dir}")
        return trainer

# BiLSTMæ¨¡å‹å®šä¹‰
class BiLSTMTranslator(nn.Module):
    """BiLSTMç¿»è¯‘æ¨¡å‹"""
    
    def __init__(self, vocab_size_src, vocab_size_tgt, embed_dim=256, 
                 hidden_dim=512, num_layers=2, dropout=0.3):
        super(BiLSTMTranslator, self).__init__()
        
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.vocab_size_tgt = vocab_size_tgt
        
        # ç¼–ç å™¨
        self.encoder_embedding = nn.Embedding(vocab_size_src, embed_dim)
        self.encoder_lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, 
                                   batch_first=True, bidirectional=True, dropout=dropout)
        
        # è§£ç å™¨
        self.decoder_embedding = nn.Embedding(vocab_size_tgt, embed_dim)
        self.decoder_lstm = nn.LSTM(embed_dim, hidden_dim * 2, num_layers, 
                                   batch_first=True, dropout=dropout)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Linear(hidden_dim * 2, hidden_dim * 2)
        self.attention_combine = nn.Linear(hidden_dim * 4, hidden_dim * 2)
        
        # è¾“å‡ºå±‚
        self.output_projection = nn.Linear(hidden_dim * 2, vocab_size_tgt)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src, tgt=None, max_length=50):
        """å‰å‘ä¼ æ’­"""
        batch_size = src.size(0)
        
        # ç¼–ç å™¨
        src_embedded = self.encoder_embedding(src)
        encoder_outputs, (hidden, cell) = self.encoder_lstm(src_embedded)
        
        # è§£ç å™¨åˆå§‹åŒ–
        decoder_hidden = hidden[-self.num_layers:].contiguous()
        decoder_cell = cell[-self.num_layers:].contiguous()
        
        if tgt is not None:
            # è®­ç»ƒæ¨¡å¼
            tgt_embedded = self.decoder_embedding(tgt)
            decoder_outputs = []
            
            for i in range(tgt.size(1)):
                decoder_input = tgt_embedded[:, i:i+1]
                decoder_output, (decoder_hidden, decoder_cell) = self.decoder_lstm(
                    decoder_input, (decoder_hidden, decoder_cell)
                )
                
                # æ³¨æ„åŠ›æœºåˆ¶
                attention_weights = torch.softmax(
                    torch.bmm(decoder_output, encoder_outputs.transpose(1, 2)), dim=2
                )
                context = torch.bmm(attention_weights, encoder_outputs)
                
                # ç»„åˆä¸Šä¸‹æ–‡å’Œè§£ç å™¨è¾“å‡º
                combined = torch.cat([decoder_output, context], dim=2)
                combined = self.attention_combine(combined)
                
                # è¾“å‡ºæŠ•å½±
                output = self.output_projection(combined)
                decoder_outputs.append(output)
            
            return torch.cat(decoder_outputs, dim=1)
        else:
            # æ¨ç†æ¨¡å¼
            decoder_outputs = []
            decoder_input = torch.zeros(batch_size, 1, dtype=torch.long, device=src.device)
            
            for i in range(max_length):
                decoder_input_embedded = self.decoder_embedding(decoder_input)
                decoder_output, (decoder_hidden, decoder_cell) = self.decoder_lstm(
                    decoder_input_embedded, (decoder_hidden, decoder_cell)
                )
                
                # æ³¨æ„åŠ›æœºåˆ¶
                attention_weights = torch.softmax(
                    torch.bmm(decoder_output, encoder_outputs.transpose(1, 2)), dim=2
                )
                context = torch.bmm(attention_weights, encoder_outputs)
                
                # ç»„åˆä¸Šä¸‹æ–‡å’Œè§£ç å™¨è¾“å‡º
                combined = torch.cat([decoder_output, context], dim=2)
                combined = self.attention_combine(combined)
                
                # è¾“å‡ºæŠ•å½±
                output = self.output_projection(combined)
                decoder_outputs.append(output)
                
                # ä¸‹ä¸€ä¸ªè¾“å…¥
                decoder_input = output.argmax(dim=-1)
            
            return torch.cat(decoder_outputs, dim=1)

# BiLSTMè®­ç»ƒå™¨
class BiLSTMTrainer:
    """BiLSTMè®­ç»ƒå™¨"""
    
    def __init__(self, vocab_size_src=10000, vocab_size_tgt=10000, embed_dim=256, 
                 hidden_dim=512, num_layers=2, dropout=0.3):
        self.vocab_size_src = vocab_size_src
        self.vocab_size_tgt = vocab_size_tgt
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.dropout = dropout
        
        self.model = None
        self.src_vocab = None
        self.tgt_vocab = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def build_vocab(self, datasets):
        """æ„å»ºè¯æ±‡è¡¨"""
        print("ğŸ”¤ æ„å»ºè¯æ±‡è¡¨...")
        
        from collections import Counter
        
        # ç»Ÿè®¡è¯é¢‘
        src_counter = Counter()
        tgt_counter = Counter()
        
        for example in datasets['train']:
            # ç®€å•çš„åˆ†è¯
            src_tokens = example['english'].lower().split()
            tgt_tokens = list(example['chinese'])  # å­—ç¬¦çº§åˆ†è¯
            
            src_counter.update(src_tokens)
            tgt_counter.update(tgt_tokens)
        
        # æ„å»ºè¯æ±‡è¡¨
        self.src_vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}
        self.tgt_vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}
        
        # æ·»åŠ é«˜é¢‘è¯
        for word, freq in src_counter.most_common(self.vocab_size_src - 4):
            if freq >= 2:
                self.src_vocab[word] = len(self.src_vocab)
        
        for word, freq in tgt_counter.most_common(self.vocab_size_tgt - 4):
            if freq >= 2:
                self.tgt_vocab[word] = len(self.tgt_vocab)
        
        print(f"âœ“ è¯æ±‡è¡¨æ„å»ºå®Œæˆ")
        print(f"æºè¯­è¨€è¯æ±‡è¡¨å¤§å°: {len(self.src_vocab)}")
        print(f"ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°: {len(self.tgt_vocab)}")
        
        return self.src_vocab, self.tgt_vocab
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        print("ğŸ”§ åˆå§‹åŒ–BiLSTMæ¨¡å‹...")
        
        self.model = BiLSTMTranslator(
            vocab_size_src=len(self.src_vocab),
            vocab_size_tgt=len(self.tgt_vocab),
            embed_dim=self.embed_dim,
            hidden_dim=self.hidden_dim,
            num_layers=self.num_layers,
            dropout=self.dropout
        ).to(self.device)
        
        print(f"âœ“ BiLSTMæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters())}")
        
    def preprocess_data(self, datasets):
        """é¢„å¤„ç†æ•°æ®"""
        print("ğŸ”„ é¢„å¤„ç†BiLSTMæ•°æ®...")
        cache_dir = "bilstm_tokenized_cache"
        cache_file = os.path.join(cache_dir, "processed_data.json")
        if os.path.exists(cache_file):
            print("ğŸ”„ åŠ è½½å·²ç¼“å­˜çš„BiLSTMé¢„å¤„ç†æ•°æ®...")
            with open(cache_file, "r", encoding="utf-8") as f:
                processed_data = json.load(f)
                return processed_data
        
        processed_data = {'train': [], 'test': []}
        
        for split in ['train', 'test']:
            for example in datasets[split]:
                # åˆ†è¯
                src_tokens = example['english'].lower().split()
                tgt_tokens = list(example['chinese'])
                
                # è½¬æ¢ä¸ºID
                src_ids = [self.src_vocab.get(token, self.src_vocab['<unk>']) for token in src_tokens]
                tgt_ids = [self.tgt_vocab['<sos>']] + \
                         [self.tgt_vocab.get(token, self.tgt_vocab['<unk>']) for token in tgt_tokens] + \
                         [self.tgt_vocab['<eos>']]
                
                # é•¿åº¦é™åˆ¶
                if len(src_ids) <= 50 and len(tgt_ids) <= 50:
                    processed_data[split].append({
                        'src': src_ids,
                        'tgt': tgt_ids
                    })
        
        print(f"âœ“ BiLSTMæ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(processed_data['train'])}")
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(processed_data['test'])}")
        # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®åˆ°æŒ‡å®šä½ç½®ï¼Œç­‰åˆ°ä¸‹æ¬¡è°ƒç”¨æ—¶ç›´æ¥åŠ è½½

        os.makedirs(cache_dir, exist_ok=True)
        print("ğŸ’¾ ç¼“å­˜BiLSTMé¢„å¤„ç†æ•°æ®...")
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(processed_data, f, ensure_ascii=False, indent=2)
        
        return processed_data
    
    def create_dataloader(self, data, batch_size=32, shuffle=True):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        def collate_fn(batch):
            src_batch = []
            tgt_batch = []
            
            for item in batch:
                src_batch.append(torch.tensor(item['src'], dtype=torch.long))
                tgt_batch.append(torch.tensor(item['tgt'], dtype=torch.long))
            
            # å¡«å……åˆ°ç›¸åŒé•¿åº¦
            from torch.nn.utils.rnn import pad_sequence

            src_batch = pad_sequence(src_batch, batch_first=True, padding_value=self.src_vocab['<pad>'])
            tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=self.tgt_vocab['<pad>'])
            
            return src_batch, tgt_batch
        
        return DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)
    
    def train(self, processed_data, epochs=10, batch_size=32, lr=0.001):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒBiLSTMæ¨¡å‹...")
        
        train_loader = self.create_dataloader(processed_data['train'], batch_size, shuffle=True)
        test_loader = self.create_dataloader(processed_data['test'], batch_size, shuffle=False)
        
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss(ignore_index=self.tgt_vocab['<pad>'])
        
        train_losses = []
        test_losses = []
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            total_loss = 0
            
            for src_batch, tgt_batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
                src_batch = src_batch.to(self.device)
                tgt_batch = tgt_batch.to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                tgt_input = tgt_batch[:, :-1]  # é™¤æœ€åä¸€ä¸ªtoken
                tgt_output = tgt_batch[:, 1:]  # é™¤ç¬¬ä¸€ä¸ªtoken
                
                outputs = self.model(src_batch, tgt_input)
                loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_output.reshape(-1))
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
            
            avg_train_loss = total_loss / len(train_loader)
            train_losses.append(avg_train_loss)
            
            # è¯„ä¼°é˜¶æ®µ
            self.model.eval()
            total_test_loss = 0
            
            with torch.no_grad():
                for src_batch, tgt_batch in test_loader:
                    src_batch = src_batch.to(self.device)
                    tgt_batch = tgt_batch.to(self.device)
                    
                    tgt_input = tgt_batch[:, :-1]
                    tgt_output = tgt_batch[:, 1:]
                    
                    outputs = self.model(src_batch, tgt_input)
                    loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_output.reshape(-1))
                    
                    total_test_loss += loss.item()
            
            avg_test_loss = total_test_loss / len(test_loader)
            test_losses.append(avg_test_loss)
            
            print(f"Epoch {epoch+1}/{epochs}")
            print(f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, æµ‹è¯•æŸå¤±: {avg_test_loss:.4f}")
        
        print("âœ“ BiLSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")
        return train_losses, test_losses

def train_lstm_model(datasets):
        # è®­ç»ƒBiLSTMæ¨¡å‹
    print("\n" + "="*50)
    print("è®­ç»ƒBiLSTMæ¨¡å‹")
    print("="*50)
    
    bilstm_trainer = BiLSTMTrainer(
        vocab_size_src=100000,
        vocab_size_tgt=10000,
        embed_dim=256,
        hidden_dim=2048,
        num_layers=2,
        dropout=0.3
    )
    
    # æ„å»ºè¯æ±‡è¡¨
    bilstm_trainer.build_vocab(datasets)
    bilstm_trainer.setup_model()
    
    # é¢„å¤„ç†æ•°æ®
    processed_data = bilstm_trainer.preprocess_data(datasets)
    
    # ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†
    small_processed = {
        'train': processed_data['train'][:500000],
        'test': processed_data['test'][:10000]
    }
    
    # è®­ç»ƒæ¨¡å‹
    train_losses, test_losses = bilstm_trainer.train(
        small_processed, 
        epochs=5, 
        batch_size=32, 
        lr=0.001
    )
    
    # ä¿å­˜æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(test_losses, label='Test Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('BiLSTM Training Progress')
    plt.legend()
    plt.savefig('bilstm_training_loss.png')
    print("âœ“ è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜ä¸º bilstm_training_loss.png")
    
    # ä¿å­˜BiLSTMæ¨¡å‹
    torch.save({
        'model_state_dict': bilstm_trainer.model.state_dict(),
        'src_vocab': bilstm_trainer.src_vocab,
        'tgt_vocab': bilstm_trainer.tgt_vocab,
        'model_config': {
            'vocab_size_src': len(bilstm_trainer.src_vocab),
            'vocab_size_tgt': len(bilstm_trainer.tgt_vocab),
            'embed_dim': bilstm_trainer.embed_dim,
            'hidden_dim': bilstm_trainer.hidden_dim,
            'num_layers': bilstm_trainer.num_layers,
            'dropout': bilstm_trainer.dropout
        }
    }, 'bilstm_translation_model.pth')
    
    print("âœ“ BiLSTMæ¨¡å‹å·²ä¿å­˜ä¸º bilstm_translation_model.pth")

def train_t5_model(datasets,model_name="t5-small",size=0.1):
     # è®­ç»ƒT5æ¨¡å‹
    print("\n" + "="*50)
    print("è®­ç»ƒT5æ¨¡å‹")
    print("="*50)

    t5_trainer = T5Trainer(model_name=model_name, max_length=512)
    t5_trainer.setup_model()
    tokenized_datasets = t5_trainer.preprocess_data(datasets)
    
    # ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†è¿›è¡Œå¿«é€Ÿè®­ç»ƒ
    small_train = tokenized_datasets["train"].select(range(int(size*len(tokenized_datasets["train"]))))
    small_test = tokenized_datasets["test"].select(range(10000))
    small_datasets = {"train": small_train, "test": small_test}
    output_dir = f"t5/{model_name}_{size}"
    t5_trainer.train(small_datasets, output_dir=output_dir, epochs=2, batch_size=256)
    
def scale_law_for_t5_model(datasets):
    train_t5_model(datasets, model_name="t5-small", size=0.001)
    #train_t5_model(datasets, model_name="t5-small", size=0.01)
    #train_t5_model(datasets, model_name="t5-small", size=0.1)
    #train_t5_model(datasets, model_name="t5-small", size=1)
    #train_t5_model(datasets, model_name="t5-base", size=0.1)
    #train_t5_model(datasets, model_name="t5-large", size=0.1)
    

# ä¸»è®­ç»ƒå‡½æ•°
def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=== è‹±ä¸­ç¿»è¯‘æ¨¡å‹è®­ç»ƒ ===")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # æ•°æ®å¤„ç†
    data_processor = DataProcessor(
        train_file="data/translation2019zh_train.json",
        valid_file="data/translation2019zh_valid.json",
        test_size=0.01
    )
    
    datasets = data_processor.load_data()
    #train_t5_model(datasets, model_name="t5-small", size=0.1)
    #train_lstm_model(datasets)
    scale_law_for_t5_model(datasets)

    
    print("\n" + "="*50)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*50)
    print("æ¨¡å‹ä¿å­˜ä½ç½®:")
    print("- T5æ¨¡å‹: t5_translation_model/")
    print("- BiLSTMæ¨¡å‹: bilstm_translation_model.pth")
    print("- è®­ç»ƒæ›²çº¿: bilstm_training_loss.png")

if __name__ == "__main__":
    main()
