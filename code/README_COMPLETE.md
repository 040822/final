# è‹±è¯‘ä¸­ç¥ç»ç½‘ç»œç¿»è¯‘ç³»ç»Ÿ

ä¸€ä¸ªé«˜æ•ˆã€å¯æ‰©å±•çš„è‹±æ–‡åˆ°ä¸­æ–‡ç¥ç»ç½‘ç»œç¿»è¯‘ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§æ¨¡å‹æ¶æ„ã€å¤šå¡è®­ç»ƒã€æ™ºèƒ½æ•°æ®åŠ è½½å’Œå®Œæ•´çš„æ¨ç†è¯„ä¼°æµç¨‹ã€‚

## ğŸš€ ç‰¹æ€§

### æ ¸å¿ƒåŠŸèƒ½
- **å¤šæ¨¡å‹æ¶æ„æ”¯æŒ**: BiLSTMã€Transformerã€è½»é‡çº§æ¨¡å‹ã€é¢„è®­ç»ƒæ¨¡å‹
- **é«˜æ•ˆæ•°æ®å¤„ç†**: æ™ºèƒ½ç¼“å­˜ã€å¹¶è¡ŒåŠ è½½ã€è‡ªåŠ¨æ•°æ®åˆ†å‰²
- **å¤šå¡è®­ç»ƒ**: é›†æˆAccelerateåº“ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
- **çµæ´»åˆ†è¯**: æ”¯æŒTransformerså’ŒSpaCyåˆ†è¯å™¨
- **å®Œæ•´æ¨ç†**: å•æ–‡æœ¬ç¿»è¯‘ã€æ‰¹é‡å¤„ç†ã€æ¨¡å‹å¯¹æ¯”
- **å¯è§†åŒ–è®­ç»ƒ**: å®æ—¶è®­ç»ƒæ›²çº¿ã€æŸå¤±ç›‘æ§ã€æ€§èƒ½åˆ†æ

### æŠ€æœ¯äº®ç‚¹
- **æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ**: é¿å…é‡å¤é¢„å¤„ç†ï¼Œå¤§å¹…æå‡æ•°æ®åŠ è½½é€Ÿåº¦
- **å†…å­˜ä¼˜åŒ–**: æ”¯æŒå¤§æ•°æ®é›†çš„åˆ†å—å¤„ç†å’Œé‡‡æ ·
- **å¤šè¿›ç¨‹å¹¶è¡Œ**: å……åˆ†åˆ©ç”¨å¤šæ ¸CPUè¿›è¡Œæ•°æ®é¢„å¤„ç†
- **æ··åˆç²¾åº¦è®­ç»ƒ**: å‡å°‘æ˜¾å­˜å ç”¨ï¼Œæå‡è®­ç»ƒé€Ÿåº¦
- **è‡ªåŠ¨æ—©åœ**: é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒèŠ‚çœè®­ç»ƒæ—¶é—´

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.0+ (å¯é€‰ï¼Œç”¨äºGPUè®­ç»ƒ)
- 8GB+ RAM (æ¨è16GB+)
- 4GB+ æ˜¾å­˜ (ç”¨äºGPUè®­ç»ƒ)

## ğŸ› ï¸ å®‰è£…

### 1. å…‹éš†ä»£ç åº“

```bash
git clone <repository-url>
cd final/code
```

### 2. å®‰è£…ä¾èµ–

```bash
# ä½¿ç”¨ä¾¿æ·è„šæœ¬
python launcher.py setup

# æˆ–æ‰‹åŠ¨å®‰è£…
pip install -r requirements.txt
```

### 3. æ•°æ®å‡†å¤‡

å°†ç¿»è¯‘æ•°æ®æ”¾ç½®åœ¨ `data/` ç›®å½•ä¸‹ï¼Œæ ¼å¼ä¸ºJSONï¼š

```json
[
  {
    "english": "Hello, how are you?",
    "chinese": "ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ"
  },
  ...
]
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¿«é€Ÿè®­ç»ƒ

```bash
# ä½¿ç”¨ä¾¿æ·è„šæœ¬è®­ç»ƒBiLSTMæ¨¡å‹
python launcher.py train bilstm

# è®­ç»ƒå…¶ä»–æ¨¡å‹
python launcher.py train transformer
python launcher.py train lightweight
python launcher.py train pretrained
```

### 2. è‡ªå®šä¹‰è®­ç»ƒ

```bash
# å®Œæ•´è®­ç»ƒå‘½ä»¤
python main_trainer.py \
    --train_data_path data/translation2019zh_train.json \
    --model_type bilstm \
    --epochs 10 \
    --batch_size 32 \
    --learning_rate 0.001 \
    --use_accelerate \
    --mixed_precision fp16 \
    --log_dir logs/bilstm \
    --model_dir models/bilstm
```

### 3. å¿«é€Ÿæ¨ç†

```bash
# ä½¿ç”¨ä¾¿æ·è„šæœ¬
python launcher.py inference models/bilstm/best_model.pt "Hello world"

# æ‰¹é‡ç¿»è¯‘
python launcher.py batch models/bilstm/best_model.pt input.txt output.txt
```

### 4. æ¨¡å‹è¯„ä¼°

```bash
# è¯„ä¼°æ¨¡å‹
python launcher.py evaluate models/bilstm/best_model.pt data/test_data.json
```

## ğŸ“Š æ¨¡å‹æ¶æ„

### 1. BiLSTMç¿»è¯‘å™¨ (BiLSTMTranslator)

- **ç¼–ç å™¨**: åŒå‘LSTMï¼Œæ•è·æºè¯­è¨€çš„åŒå‘ä¸Šä¸‹æ–‡
- **è§£ç å™¨**: å•å‘LSTM + æ³¨æ„åŠ›æœºåˆ¶
- **ç‰¹ç‚¹**: å‚æ•°é‡é€‚ä¸­ï¼Œè®­ç»ƒç¨³å®šï¼Œé€‚åˆä¸­å°è§„æ¨¡æ•°æ®

```python
# æ¨¡å‹é…ç½®ç¤ºä¾‹
model_config = {
    'embedding_dim': 512,
    'hidden_dim': 512,
    'num_layers': 2,
    'dropout': 0.1
}
```

### 2. Transformerç¿»è¯‘å™¨ (TransformerTranslator)

- **æ¶æ„**: æ ‡å‡†Transformer encoder-decoderç»“æ„
- **æ³¨æ„åŠ›**: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- **ç‰¹ç‚¹**: å¹¶è¡Œè®¡ç®—ï¼Œé•¿è·ç¦»ä¾èµ–å»ºæ¨¡èƒ½åŠ›å¼º

```python
# æ¨¡å‹é…ç½®ç¤ºä¾‹
model_config = {
    'd_model': 512,
    'nhead': 8,
    'num_layers': 6,
    'dropout': 0.1
}
```

### 3. è½»é‡çº§ç¿»è¯‘å™¨ (LightweightTranslator)

- **ç¼–ç å™¨**: é¢„è®­ç»ƒçš„å¤šè¯­è¨€BERT
- **è§£ç å™¨**: ç®€åŒ–çš„Transformerè§£ç å™¨
- **ç‰¹ç‚¹**: åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†ï¼Œè®­ç»ƒå¿«é€Ÿï¼Œæ•ˆæœå¥½

### 4. é¢„è®­ç»ƒç¿»è¯‘å™¨ (PretrainedTranslator)

- **åŸºç¡€**: Helsinki-NLP/opus-mt-en-zh æˆ–å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹
- **å¾®è°ƒ**: æ”¯æŒå…¨å‚æ•°æˆ–éƒ¨åˆ†å‚æ•°å¾®è°ƒ
- **ç‰¹ç‚¹**: é›¶æ ·æœ¬èƒ½åŠ›å¼ºï¼Œé€‚åˆèµ„æºå—é™åœºæ™¯

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. æ•°æ®åŠ è½½ä¼˜åŒ–

```python
# æ™ºèƒ½æ•°æ®åŠ è½½å™¨é…ç½®
from smart_dataset import create_smart_dataloaders

train_loader, val_loader, test_loader, vocab_info = create_smart_dataloaders(
    train_path='data/train.json',
    batch_size=32,
    tokenizer_type='transformers',
    cache_dir='cache',
    sample_ratio=0.1,  # ä½¿ç”¨10%çš„æ•°æ®è¿›è¡Œå¿«é€Ÿå®éªŒ
    num_workers=4,
    test_split=0.1,
    val_split=0.1
)
```

### 2. å¤šå¡è®­ç»ƒ

```python
# å¯ç”¨Accelerate
python main_trainer.py \
    --use_accelerate \
    --mixed_precision fp16 \
    --gradient_accumulation_steps 2
```

### 3. æ¨¡å‹å¯¹æ¯”

```python
# åˆ›å»ºæ¨¡å‹é…ç½®æ–‡ä»¶
model_configs = [
    {
        "name": "BiLSTM",
        "model_path": "models/bilstm/best_model.pt",
        "config_path": "configs/bilstm_config.json"
    },
    {
        "name": "Transformer",
        "model_path": "models/transformer/best_model.pt",
        "config_path": "configs/transformer_config.json"
    }
]

# æ‰§è¡Œå¯¹æ¯”
python inference_new.py compare \
    --model_configs model_configs.json \
    --input_texts test_sentences.txt \
    --output_file comparison_results.json
```

### 4. è®­ç»ƒç›‘æ§

ç³»ç»Ÿä¼šè‡ªåŠ¨ç”Ÿæˆï¼š
- **TensorBoardæ—¥å¿—**: å®æ—¶è®­ç»ƒæ›²çº¿
- **è®­ç»ƒå›¾è¡¨**: lossã€BLEUåˆ†æ•°ã€å­¦ä¹ ç‡å˜åŒ–
- **æ£€æŸ¥ç‚¹**: è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
- **æ—¥å¿—æ–‡ä»¶**: è¯¦ç»†çš„è®­ç»ƒè¿‡ç¨‹è®°å½•

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–

```python
# å¤§æ•°æ®é›†å¤„ç†
--sample_ratio 0.1          # ä½¿ç”¨10%æ•°æ®å¿«é€Ÿå®éªŒ
--gradient_accumulation_steps 4  # æ¢¯åº¦ç´¯ç§¯ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
--mixed_precision fp16       # æ··åˆç²¾åº¦è®­ç»ƒ
```

### 2. é€Ÿåº¦ä¼˜åŒ–

```python
# å¹¶è¡Œæ•°æ®åŠ è½½
--num_workers 4             # 4ä¸ªè¿›ç¨‹å¹¶è¡ŒåŠ è½½æ•°æ®
--cache_dir cache           # å¯ç”¨ç¼“å­˜ç³»ç»Ÿ
```

### 3. è®­ç»ƒç­–ç•¥

```python
# æ™ºèƒ½è®­ç»ƒç­–ç•¥
--early_stopping 5          # æ—©åœæœºåˆ¶
--use_scheduler             # å­¦ä¹ ç‡è°ƒåº¦
--warmup_ratio 0.1          # é¢„çƒ­ç­–ç•¥
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. ç ”ç©¶å®éªŒ

```bash
# å¿«é€ŸåŸå‹éªŒè¯
python launcher.py train bilstm data/small_dataset.json 3

# ä¸åŒæ¨¡å‹å¯¹æ¯”
python launcher.py train transformer
python launcher.py train lightweight
```

### 2. ç”Ÿäº§éƒ¨ç½²

```bash
# å®Œæ•´è®­ç»ƒ
python main_trainer.py \
    --train_data_path data/full_dataset.json \
    --model_type lightweight \
    --epochs 20 \
    --batch_size 64 \
    --use_accelerate \
    --mixed_precision fp16

# æ€§èƒ½è¯„ä¼°
python launcher.py evaluate models/lightweight/best_model.pt data/test.json
```

### 3. æ¨¡å‹åˆ†æ

```bash
# å¤šæ¨¡å‹å¯¹æ¯”
python inference_new.py compare \
    --model_configs configs/all_models.json \
    --input_texts test_sentences.txt \
    --output_file comparison.json \
    --eval_data data/test.json
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
code/
â”œâ”€â”€ main_trainer.py          # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ models.py               # æ¨¡å‹æ¶æ„å®šä¹‰
â”œâ”€â”€ smart_dataset.py        # æ™ºèƒ½æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ utils.py                # å·¥å…·å‡½æ•°
â”œâ”€â”€ inference_new.py        # æ¨ç†å’Œè¯„ä¼°
â”œâ”€â”€ launcher.py             # ä¾¿æ·å¯åŠ¨è„šæœ¬
â”œâ”€â”€ requirements.txt        # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md              # è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ translation2019zh_train.json
â”‚   â””â”€â”€ translation2019zh_valid.json
â”œâ”€â”€ models/                # æ¨¡å‹ä¿å­˜ç›®å½•
â”œâ”€â”€ logs/                  # æ—¥å¿—ç›®å½•
â”œâ”€â”€ cache/                 # ç¼“å­˜ç›®å½•
â””â”€â”€ configs/               # é…ç½®æ–‡ä»¶ç›®å½•
```

## ğŸ” å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¤„ç†å¤§æ•°æ®é›†ï¼Ÿ

A: ä½¿ç”¨é‡‡æ ·å’Œç¼“å­˜æœºåˆ¶ï¼š

```bash
python main_trainer.py \
    --sample_ratio 0.1 \
    --cache_dir cache \
    --num_workers 4
```

### Q2: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

A: ä½¿ç”¨æ··åˆç²¾åº¦å’Œæ¢¯åº¦ç´¯ç§¯ï¼š

```bash
python main_trainer.py \
    --mixed_precision fp16 \
    --gradient_accumulation_steps 4 \
    --batch_size 16
```

### Q3: å¦‚ä½•æå‡è®­ç»ƒé€Ÿåº¦ï¼Ÿ

A: å¯ç”¨å¤šå¡è®­ç»ƒå’Œä¼˜åŒ–æ•°æ®åŠ è½½ï¼š

```bash
python main_trainer.py \
    --use_accelerate \
    --mixed_precision fp16 \
    --num_workers 4
```

### Q4: æ¨¡å‹æ•ˆæœä¸å¥½æ€ä¹ˆåŠï¼Ÿ

A: å°è¯•ä¸åŒæ¨¡å‹å’Œè°ƒæ•´è¶…å‚æ•°ï¼š

```bash
# å°è¯•é¢„è®­ç»ƒæ¨¡å‹
python launcher.py train pretrained

# è°ƒæ•´å­¦ä¹ ç‡
python main_trainer.py --learning_rate 0.0001

# å¢åŠ è®­ç»ƒè½®æ¬¡
python main_trainer.py --epochs 20
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Forké¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. å‘èµ·Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- é‚®ç®±: [your-email@example.com]
- é¡¹ç›®ä¸»é¡µ: [repository-url]

---

â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸€ä¸ªStarï¼
